{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"initial_id\",\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import ollama\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, f1_score\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- 1. CONFIGURAZIONE MODELLI ---\\n\",\n",
    "    \"# Definiamo i modelli e la loro dimensione in Miliardi di parametri (B)\\n\",\n",
    "    \"# Assicurati di aver fatto 'ollama pull <nome_modello>' per ognuno!\\n\",\n",
    "    \"models_config = {\\n\",\n",
    "    \"    \\\"tinyllama\\\": 1.1,   # 1.1B\\n\",\n",
    "    \"    \\\"gemma:2b\\\": 2.0,    # 2B\\n\",\n",
    "    \"    \\\"phi3\\\": 3.8,        # 3.8B (phi3 mini)\\n\",\n",
    "    \"    \\\"llama3\\\": 8.0       # 8B\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# File di cache per salvare i risultati progressivamente\\n\",\n",
    "    \"SCALING_CACHE_FILE = \\\"scaling_laws_results.csv\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- 2. FUNZIONI DI SUPPORTO (Riutilizziamo quelle della Parte 3 per coerenza) ---\\n\",\n",
    "    \"def create_sentiment_prompt(comment: str) -> str:\\n\",\n",
    "    \"    return f\\\"\\\"\\\"Analyze the sentiment of the text below.\\n\",\n",
    "    \"Return only one word: positive, negative, or neutral.\\n\",\n",
    "    \"Text: \\\"{comment}\\\"\\n\",\n",
    "    \"Sentiment:\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def parse_llm_response(response: str) -> str:\\n\",\n",
    "    \"    if not response:\\n\",\n",
    "    \"        return 'neutral'\\n\",\n",
    "    \"    text = response.lower().strip()\\n\",\n",
    "    \"    match = re.search(r'\\\\b(positive|negative|neutral)\\\\b', text)\\n\",\n",
    "    \"    if match:\\n\",\n",
    "    \"        return match.group(1)\\n\",\n",
    "    \"    return 'neutral'\\n\",\n",
    "    \"\\n\",\n",
    "    \"def classify_row(row, model_name):\\n\",\n",
    "    \"    \\\"\\\"\\\"Esegue la classificazione per una singola riga e un dato modello.\\\"\\\"\\\"\\n\",\n",
    "    \"    prompt = create_sentiment_prompt(row['body'])\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': prompt}])\\n\",\n",
    "    \"        raw_output = response['message']['content']\\n\",\n",
    "    \"        elapsed = time.time() - start_time\\n\",\n",
    "    \"        return parse_llm_response(raw_output), elapsed\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error {model_name}: {e}\\\")\\n\",\n",
    "    \"        return \\\"error\\\", 0.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- 3. LOOP DI INFERENZA (SCALING) ---\\n\",\n",
    "    \"# Se esiste giÃ  un file di risultati parziali, lo carichiamo per non ripartire da zero\\n\",\n",
    "    \"if os.path.exists(SCALING_CACHE_FILE):\\n\",\n",
    "    \"    print(f\\\"Trovati risultati precedenti in {SCALING_CACHE_FILE}. Caricamento...\\\")\\n\",\n",
    "    \"    results_df = pd.read_csv(SCALING_CACHE_FILE)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    results_df = pd.DataFrame()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Creiamo una lista per i nuovi risultati\\n\",\n",
    "    \"new_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Inizio Benchmark su {len(df_eval)} commenti per {len(models_config)} modelli...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model_name, size_b in models_config.items():\\n\",\n",
    "    \"    print(f\\\"\\\\nðŸ¤– Testing Model: {model_name} ({size_b}B Params)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Controlliamo se abbiamo giÃ  processato questo modello (per evitare duplicati se riavvii)\\n\",\n",
    "    \"    if not results_df.empty and model_name in results_df['model'].unique():\\n\",\n",
    "    \"        print(f\\\"  -> GiÃ  completato. Skipping.\\\")\\n\",\n",
    "    \"        continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"    model_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Iteriamo su df_eval (il dataset con ground truth)\\n\",\n",
    "    \"    for index, row in tqdm(df_eval.iterrows(), total=len(df_eval), desc=f\\\"Inferenza {model_name}\\\"):\\n\",\n",
    "    \"        pred_label, pred_time = classify_row(row, model_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        model_results.append({\\n\",\n",
    "    \"            'model': model_name,\\n\",\n",
    "    \"            'size_b': size_b,\\n\",\n",
    "    \"            'original_index': index, # Utile per join futuri\\n\",\n",
    "    \"            'true_label': row['manual_label'],\\n\",\n",
    "    \"            'pred_label': pred_label,\\n\",\n",
    "    \"            'inference_time': pred_time\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Salvataggio intermedio su CSV dopo ogni modello\\n\",\n",
    "    \"    batch_df = pd.DataFrame(model_results)\\n\",\n",
    "    \"    # Se Ã¨ il primo modello, scrive header, altrimenti appende\\n\",\n",
    "    \"    write_header = not os.path.exists(SCALING_CACHE_FILE)\\n\",\n",
    "    \"    batch_df.to_csv(SCALING_CACHE_FILE, mode='a', header=write_header, index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Aggiorniamo il dataframe in memoria\\n\",\n",
    "    \"    results_df = pd.concat([results_df, batch_df], ignore_index=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nâœ… Benchmark completato.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- 4. CALCOLO METRICHE E ANALISI ---\\n\",\n",
    "    \"# Ricarichiamo tutto per sicurezza\\n\",\n",
    "    \"full_results = pd.read_csv(SCALING_CACHE_FILE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filtriamo eventuali errori\\n\",\n",
    "    \"full_results = full_results[full_results['pred_label'] != 'error']\\n\",\n",
    "    \"\\n\",\n",
    "    \"performance_data = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model in models_config.keys():\\n\",\n",
    "    \"    subset = full_results[full_results['model'] == model]\\n\",\n",
    "    \"    if len(subset) == 0:\\n\",\n",
    "    \"        continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"    acc = accuracy_score(subset['true_label'], subset['pred_label'])\\n\",\n",
    "    \"    f1 = f1_score(subset['true_label'], subset['pred_label'], average='weighted')\\n\",\n",
    "    \"    avg_time = subset['inference_time'].mean()\\n\",\n",
    "    \"    size = models_config[model]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    performance_data.append({\\n\",\n",
    "    \"        'Model': model,\\n\",\n",
    "    \"        'Size (B)': size,\\n\",\n",
    "    \"        'Accuracy': acc,\\n\",\n",
    "    \"        'F1-Score': f1,\\n\",\n",
    "    \"        'Avg Time (s)': avg_time\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_perf = pd.DataFrame(performance_data)\\n\",\n",
    "    \"print(\\\"\\\\nRIEPILOGO PERFORMANCE:\\\")\\n\",\n",
    "    \"print(df_perf)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# --- 5. VISUALIZZAZIONE GRAFICA ---\\n\",\n",
    "    \"sns.set_style(\\\"whitegrid\\\")\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Grafico 1: Size vs Accuracy\\n\",\n",
    "    \"sns.lineplot(data=df_perf, x='Size (B)', y='Accuracy', marker='o', ax=axes[0], color='b', label='Accuracy')\\n\",\n",
    "    \"# Aggiungiamo etichette ai punti\\n\",\n",
    "    \"for i, row in df_perf.iterrows():\\n\",\n",
    "    \"    axes[0].text(row['Size (B)'], row['Accuracy'] + 0.01, row['Model'], fontsize=10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[0].set_title('Scaling Law: Model Size vs Accuracy', fontsize=14)\\n\",\n",
    "    \"axes[0].set_xlabel('Parametri Modello (Miliardi)', fontsize=12)\\n\",\n",
    "    \"axes[0].set_ylabel('Accuracy', fontsize=12)\\n\",\n",
    "    \"axes[0].set_ylim(0, 1.05)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Grafico 2: Size vs Inference Time\\n\",\n",
    "    \"sns.barplot(data=df_perf, x='Model', y='Avg Time (s)', ax=axes[1], palette='viridis')\\n\",\n",
    "    \"# Ordinamento per dimensione per coerenza visiva\\n\",\n",
    "    \"# (Se i modelli nel dataframe non sono ordinati, il barplot usa l'ordine di apparizione o alfabetico)\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[1].set_title('Costo Computazionale: Tempo Medio di Inferenza', fontsize=14)\\n\",\n",
    "    \"axes[1].set_xlabel('Modello', fontsize=12)\\n\",\n",
    "    \"axes[1].set_ylabel('Secondi per commento', fontsize=12)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig('scaling_laws_plot.png')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "c4de275807f5f1f4"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
