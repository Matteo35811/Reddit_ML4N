{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SECTION 4 - SENTIMENT ANALYSIS\n",
    "\n",
    "### PART 1 - CLASSICAL ANALYSIS\n",
    "Try using VADER from NLTK or HuggingFace transformers. The aim is to classify each comment in the supervised dataset as positive,\n",
    "negative, or neutral.\n",
    "\n",
    "a. Note that for sentiment analysis removing some stopwords (e.g., “not”, “never”, “no”) may\n",
    "be harmful. Slightly adjust your preprocessing pipeline if you deem it useful.\n",
    "\n",
    "b. What is the overall sentiment distribution across all comments?\n",
    "\n",
    "c. Visualize sentiment distribution per subreddit using bar charts or heatmaps.\n",
    "\n",
    "d. Does sentiment correlate with gender?"
   ],
   "id": "a4773dd9892c2ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Setup and Data",
   "id": "9ccf2fac926728a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:56:17.293920Z",
     "start_time": "2026-01-08T12:56:15.367376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn nltk scipy"
   ],
   "id": "b93d61b46723d96b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: click in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:56:21.206856Z",
     "start_time": "2026-01-08T12:56:17.304908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib. pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment. vader import SentimentIntensityAnalyzer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ],
   "id": "c8c9fc7cd286695d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rocca\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:56:22.121171Z",
     "start_time": "2026-01-08T12:56:21.248750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the supervised dataset\n",
    "# For sentiment analysis, we need the original text with stopwords intact\n",
    "\n",
    "df_supervised = pd.read_csv('../data/data_supervised.csv')\n",
    "df_target = pd.read_csv('../data/target_supervised.csv')\n",
    "\n",
    "print(f\"Supervised dataset shape: {df_supervised. shape}\")\n",
    "print(f\"Target dataset shape: {df_target.shape}\")\n",
    "print(f\"\\nColumns in supervised data: {df_supervised.columns.tolist()}\")\n",
    "print(f\"Columns in target data: {df_target.columns.tolist()}\")"
   ],
   "id": "7f174627e6466592",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised dataset shape: (296042, 4)\n",
      "Target dataset shape: (5000, 2)\n",
      "\n",
      "Columns in supervised data: ['author', 'subreddit', 'created_utc', 'body']\n",
      "Columns in target data: ['author', 'gender']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:56:57.956649Z",
     "start_time": "2026-01-08T12:56:57.939018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preview the data\n",
    "print(\"Sample comments:\")\n",
    "df_supervised.head()"
   ],
   "id": "4f47ee846f4ab90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample comments:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          author          subreddit   created_utc  \\\n",
       "0    Shamus_Aran       mylittlepony  1.388534e+09   \n",
       "1       Riddance                sex  1.388534e+09   \n",
       "2  Secret_Wizard       DragonsDogma  1.388534e+09   \n",
       "3   Penultimatum  malefashionadvice  1.388534e+09   \n",
       "4      7-SE7EN-7      todayilearned  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I don't think we'd get nearly as much fanficti...  \n",
       "1  Thanks. I made it up, that's how I got over my...  \n",
       "2  Are you sure you aren't confusing Cyclops (the...  \n",
       "3                             dont do this to me bro  \n",
       "4        That's what we do when we can't find a mate  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>dont do this to me bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>That's what we do when we can't find a mate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. PREPROCESSING\n",
    "Remember that is important to NOT remove negation words like \"not\", \"never\", \"no\" as they change the sentiment of a sentence.\n",
    "\n",
    "Thus we apply minimal preprocessing that preserves sentiment-bearing words."
   ],
   "id": "415f4e68d65ba4ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:04:04.932732Z",
     "start_time": "2026-01-08T13:04:00.664609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "def preprocess_for_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Decode HTML entities (e.g., &amp; -> &)\n",
    "    text = html.unescape(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove subreddit and user references (r/...  and u/... )\n",
    "    text = re.sub(r'r/\\w+|u/\\w+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df_supervised['body_sentiment'] = df_supervised['body']. apply(preprocess_for_sentiment)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Empty bodies after preprocessing: {(df_supervised['body_sentiment'] == '').sum()}\")"
   ],
   "id": "8050f54a5b65d143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "Empty bodies after preprocessing: 987\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:04:13.185470Z",
     "start_time": "2026-01-08T13:04:13.122085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show comparison between original and preprocessed text\n",
    "comparison_df = df_supervised[['body', 'body_sentiment']].head(5)\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"--- Comment {idx} ---\")\n",
    "    print(f\"Original: {row['body'][:200]}...\" if len(str(row['body'])) > 200 else f\"Original: {row['body']}\")\n",
    "    print(f\"Preprocessed: {row['body_sentiment'][:200]}...\" if len(row['body_sentiment']) > 200 else f\"Preprocessed:  {row['body_sentiment']}\")\n",
    "    print()"
   ],
   "id": "9db92d185e064697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comment 0 ---\n",
      "Original: I don't think we'd get nearly as much fanfiction and pictures shipping Ban-Ban and Lyro. Just saying.\n",
      "Preprocessed:  I don't think we'd get nearly as much fanfiction and pictures shipping Ban-Ban and Lyro. Just saying.\n",
      "\n",
      "--- Comment 1 ---\n",
      "Original: Thanks. I made it up, that's how I got over my first heart break. \n",
      "Preprocessed:  Thanks. I made it up, that's how I got over my first heart break.\n",
      "\n",
      "--- Comment 2 ---\n",
      "Original: Are you sure you aren't confusing Cyclops (the easiest boss monster) for Ogres? I'm talking about [these guys](http://i.imgur.com/c3YKPdI.jpg)\n",
      "\n",
      "Maybe I'm just a bad player... But every time I faced on...\n",
      "Preprocessed: Are you sure you aren't confusing Cyclops (the easiest boss monster) for Ogres? I'm talking about [these guys]( Maybe I'm just a bad player... But every time I faced one on my first playthrough, all m...\n",
      "\n",
      "--- Comment 3 ---\n",
      "Original: dont do this to me bro\n",
      "Preprocessed:  dont do this to me bro\n",
      "\n",
      "--- Comment 4 ---\n",
      "Original: That's what we do when we can't find a mate\n",
      "Preprocessed:  That's what we do when we can't find a mate\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Sentiment Analysis with VADER",
   "id": "fbd3004b7e86ad8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
