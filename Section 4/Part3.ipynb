{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4 - PART 3: LLM ZERO-SHOT CLASSIFICATION\n",
    "\n",
    "Sentiment Analysis - LLM â€“ zero-shot: Try zero-shot classification using a (small) LLM model like tinyllama (1.1B) or qwen3:1.7b.\n",
    "\n",
    "**Obiettivi:**\n",
    "1. Configuring Ollama pipeline (es. TinyLlama, Qwen).\n",
    "2. Claassify a reddit comment subset.\n",
    "3. Comparing the results with VADER (baseline) and with manual label (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T13:52:16.517950200Z",
     "start_time": "2026-01-16T13:52:07.084241800Z"
    }
   },
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn tqdm ollama nltk\n",
    "\n",
    "# Important OLLAMA:\n",
    "# 1. Install\n",
    "# 2. 'ollama serve'\n",
    "# 3. 'ollama pull tinyllama'\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ollama) (2.12.4)\n",
      "Requirement already satisfied: click in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rocca\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import ollama\n",
    "import os\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Verufy ollama connection\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(f\"\\nAvailable models: {[m['model'] for m in models.get('models', [])]}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ ERROR OLLAMA: {e}\")\n",
    "    print(\"Is 'ollama serve' executing on terminal?\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Loading and Sampling Data\n",
    "Let's select a representative subset of comments from different subreddits to test the generalization capacity of the model."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_supervised = pd.read_csv('../data/data_supervised.csv')\n",
    "\n",
    "# stratified\n",
    "np.random.seed(42)\n",
    "\n",
    "top_subreddits = df_supervised['subreddit'].value_counts().head(10).index.tolist()\n",
    "sample_dfs = []\n",
    "\n",
    "for sub in top_subreddits:\n",
    "    sub_df = df_supervised[df_supervised['subreddit'] == sub]\n",
    "    if 'data_supervised.csv' in locals().get('df_supervised', ''):\n",
    "         sub_df = sub_df[sub_df['body'].str.len().between(50, 500)]\n",
    "    \n",
    "    # sample_size = min(5, len(sub_df))  # Let's sample 5 comments per sub to have better speed\n",
    "    sample_size = min(10, len(sub_df))  # Let's sample 10 comments instead per sub to have better speed\n",
    "    sample_dfs.append(sub_df.sample(n=sample_size, random_state=42))\n",
    "\n",
    "df_sample = pd.concat(sample_dfs, ignore_index=True)\n",
    "print(f\"Sampled comments: {len(df_sample)}\")\n",
    "df_sample.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Defining Pipeline LLM (Ollama)\n",
    "\n",
    "Let's define the prompt and the function to question the local model (LM)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''============================ better be concise then this type of prompt ============\n",
    "def create_sentiment_prompt(comment: str) -> str:\n",
    "    return f\"\"\"You are a sentiment classifier. Classify the following Reddit comment into exactly ONE of these three categories: positive, negative, or neutral.\n",
    "\n",
    "Guidelines:\n",
    "- \"positive\": Expresses happiness, satisfaction, gratitude, excitement, support.\n",
    "- \"negative\": Expresses anger, frustration, sadness, criticism, disappointment.\n",
    "- \"neutral\": Informational, factual, or no clear emotion.\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Respond with ONLY ONE WORD: positive, negative, or neutral.\n",
    "Classification:\"\"\"\n",
    "'''\n",
    "def create_sentiment_prompt(comment: str) -> str:\n",
    "    return f\"\"\"Analyze the sentiment of the text below.\n",
    "Return only one word: positive, negative, or neutral.\n",
    "Text: \"{comment}\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "def parse_llm_response(response: str) -> str:\n",
    "    if not response:\n",
    "        return 'neutral'\n",
    "    text = response.lower().strip()\n",
    "    # we use re to avoid responses like \"not positive\" interpreted as \"positive\"\n",
    "    match = re.search(r'\\b(positive|negative|neutral)\\b', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return 'neutral' # Default fallback\n",
    "\n",
    "def classify_sentiment_llm(comment: str, model: str = \"tinyllama\") -> dict:\n",
    "    prompt = create_sentiment_prompt(comment)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "        raw = resp['message']['content']\n",
    "        return {\n",
    "            'label': parse_llm_response(raw),\n",
    "            'raw': raw,\n",
    "            'time': time.time() - start,\n",
    "            'success': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'label': 'error', 'raw': str(e), 'time': 0, 'success': False}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Classification execution\n",
    " Let's do inference on data sample."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Nome del file dove salveremo i risultati per non ricalcolarli\n",
    "CACHE_FILE = \"llm_inference_results.csv\"\n",
    "MODEL_NAME = \"tinyllama\"\n",
    "\n",
    "# --- LOGICA DI CACHING ---\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    # CASO A: Il file esiste -> Carichiamo e SALTANDO il loop lento\n",
    "    print(f\"âœ… Trovato file cache '{CACHE_FILE}'. Caricamento dati esistenti...\")\n",
    "    df_sample = pd.read_csv(CACHE_FILE)\n",
    "\n",
    "else:\n",
    "    # CASO B: Il file NON esiste -> Eseguiamo il loop (LENTO)\n",
    "    print(f\"ðŸš€ Cache non trovata. Inizio classificazione con {MODEL_NAME}...\")\n",
    "\n",
    "    results = []\n",
    "    # --- IL TUO CICLO ORIGINALE ---\n",
    "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "        res = classify_sentiment_llm(row['body'], model=MODEL_NAME)\n",
    "        results.append(res)\n",
    "\n",
    "    # Assegnazione risultati al DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_sample['llm_label'] = df_results['label'].values\n",
    "    df_sample['llm_time'] = df_results['time'].values\n",
    "\n",
    "    # SALVATAGGIO: Creiamo il file per la prossima volta\n",
    "    df_sample.to_csv(CACHE_FILE, index=False)\n",
    "    print(f\"ðŸ’¾ Risultati salvati in '{CACHE_FILE}'. Al prossimo avvio non dovrai ricalcolare.\")\n",
    "\n",
    "# Stampa di controllo (funziona in entrambi i casi)\n",
    "print(\"\\nDistribuzione Predizioni LLM:\")\n",
    "print(df_sample['llm_label'].value_counts())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Human-in-the-Loop: Etichettatura Manuale\n",
    "\n",
    "**PASSO CRITICO:** Per valutare il modello, abbiamo bisogno della \"Ground Truth\".\n",
    "1. Il codice esporta un CSV.\n",
    "2. **Tu** devi aprire il CSV, riempire la colonna `manual_label` (positive/negative/neutral) e salvare.\n",
    "3. Il codice poi ricarica il file etichettato."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. ESPORTAZIONE\n",
    "filename = 'comments_for_manual_labeling.csv'\n",
    "export_cols = ['subreddit', 'body']\n",
    "df_export = df_sample[export_cols].copy()\n",
    "df_export['manual_label'] = '' # Colonna vuota da riempire\n",
    "df_export.to_csv(filename, index=True)\n",
    "\n",
    "print(f\"âœ“ File '{filename}' creato.\")\n",
    "print(\"ISTRUZIONI:\")\n",
    "print(\"1. Apri il CSV in Excel/Sheets.\")\n",
    "print(\"2. Riempi la colonna 'manual_label' con: positive, negative, o neutral.\")\n",
    "print(\"3. Salva il file.\")\n",
    "print(\"4. Esegui la cella successiva SOLO dopo aver salvato.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. IMPORTAZIONE (Esegui dopo aver etichettato manualmente)\n",
    "# Simulo etichettatura manuale automatica per far funzionare il notebook demo\n",
    "# RIMUOVI QUESTO BLOCCO 'IF' NEL PROGETTO REALE E USA IL 'TRY/EXCEPT' SOTTO\n",
    "    # CODICE REALE DA USARE:\n",
    "filename = 'comments_for_manual_labeling.csv'\n",
    "try:\n",
    "    df_labeled = pd.read_csv(filename, index_col=0)\n",
    "    df_sample['manual_label'] = df_labeled['manual_label']\n",
    "    print(\"âœ“ Etichette manuali caricate con successo!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Errore caricamento: {e}\")\n",
    "    print(\"Assicurati di aver salvato il CSV e non aver cambiato l'indice.\")\n",
    "\n",
    "# Filtra solo quelli con etichette valide\n",
    "valid_labels = ['positive', 'negative', 'neutral']\n",
    "df_eval = df_sample[df_sample['manual_label'].isin(valid_labels)].copy()\n",
    "print(f\"Righe valide per la valutazione: {len(df_eval)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Baseline: VADER\n",
    "\n",
    "Eseguiamo anche VADER per avere un termine di paragone classico."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader(text):\n",
    "    score = sia.polarity_scores(str(text))['compound']\n",
    "    if score >= 0.05: return 'positive'\n",
    "    if score <= -0.05: return 'negative'\n",
    "    return 'neutral'\n",
    "\n",
    "df_eval['vader_label'] = df_eval['body'].apply(get_vader)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Risultati e Confronto\n",
    "\n",
    "Confrontiamo le metriche di LLM e VADER rispetto alle etichette manuali."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_true = df_eval['manual_label']\n",
    "y_llm = df_eval['llm_label']\n",
    "y_vader = df_eval['vader_label']\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "\n",
    "print(\"--- LLM ACCURACY ---\")\n",
    "acc_llm = accuracy_score(y_true, y_llm)\n",
    "print(f\"{acc_llm:.2%}\")\n",
    "\n",
    "print(\"\\n--- VADER ACCURACY ---\")\n",
    "acc_vader = accuracy_score(y_true, y_vader)\n",
    "print(f\"{acc_vader:.2%}\")\n",
    "\n",
    "# Visualizzazione Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_true, y_llm, labels=labels), annot=True, fmt='d', \n",
    "            xticklabels=labels, yticklabels=labels, cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(f'LLM (Acc: {acc_llm:.2%})')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_true, y_vader, labels=labels), annot=True, fmt='d', \n",
    "            xticklabels=labels, yticklabels=labels, cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(f'VADER (Acc: {acc_vader:.2%})')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#   added\n",
    "from sklearn.metrics import classification_report\n",
    "# Assicuriamoci di usare le stesse etichette per tutti\n",
    "target_categories = ['positive', 'neutral', 'negative']\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DETTAGLIO PERFORMANCE LLM\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(y_true, y_llm, labels=target_categories))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DETTAGLIO PERFORMANCE VADER (Baseline)\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(y_true, y_vader, labels=target_categories))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## FINAL CONCLUSIONS\n",
    "From the last execution, we can definitely see that the small Model LLM (1.1B) TinyLLAMa in Zero-shot, suffer a lot an evident \"positive bias\" and surely not enough capacity to understand the \"neutral class\".\n",
    "VADER on the other hand, even being a fixed rule system, performs a lot better in this \"low-cost\" scenary..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
