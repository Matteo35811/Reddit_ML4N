{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T12:39:49.322292200Z",
     "start_time": "2026-01-18T12:39:49.201747200Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "36501a69fa9379ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T12:41:52.656788700Z",
     "start_time": "2026-01-18T12:39:49.323401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# STEP 1+2 CORRETTO: Load data, split PRIMA, poi vettorizza\n",
    "# =============================================================================\n",
    "import sys\n",
    "sys.path.append(\"../source\")\n",
    "from src import stratified_split, truncated_svd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 1. Load cleaned text data\n",
    "df_supervised = pd.read_csv(\"./clean_supervised.csv\")\n",
    "df_supervised = df_supervised.dropna(subset=['body_clean'])\n",
    "\n",
    "# 2. Group by author\n",
    "df_grouped = df_supervised.groupby('author')['body_clean'].apply(\" \".join).reset_index()\n",
    "df_grouped = df_grouped.rename(columns={'body_clean': 'body'})\n",
    "\n",
    "# 3. Load and match labels\n",
    "target_df = pd.read_csv(\"./Data/target_supervised.csv\")\n",
    "labels = []\n",
    "for auth in df_grouped['author']:\n",
    "    match = target_df[target_df['author'] == auth]\n",
    "    if not match.empty:\n",
    "        labels.append(match['gender'].iloc[0])\n",
    "    else:\n",
    "        labels.append(None)\n",
    "\n",
    "df_grouped['label'] = labels\n",
    "df_grouped = df_grouped.dropna(subset=['label'])\n",
    "print(f\"Dataset size: {len(df_grouped)} authors\")\n",
    "\n",
    "# 4. SPLIT PRIMA della vettorizzazione! (questo evita data leakage)\n",
    "X_text = df_grouped['body'].values\n",
    "y = df_grouped['label'].values\n",
    "\n",
    "X_text_train, X_text_val, X_text_test, y_train, y_val, y_test = stratified_split(X_text, y)\n",
    "print(f\"Train: {len(X_text_train)}, Val: {len(X_text_val)}, Test: {len(X_text_test)}\")\n",
    "\n",
    "# 5. BoW - fit SOLO su train\n",
    "bow_vectorizer = CountVectorizer(min_df=5, max_df=0.99)\n",
    "X_bow_train = bow_vectorizer.fit_transform(X_text_train)\n",
    "X_bow_val = bow_vectorizer.transform(X_text_val)\n",
    "X_bow_test = bow_vectorizer.transform(X_text_test)\n",
    "print(f\"BoW vocabulary size: {len(bow_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# 6. TF-IDF - fit SOLO su train\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.99)\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_text_train)\n",
    "X_tfidf_val = tfidf_vectorizer.transform(X_text_val)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_text_test)\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# 7. SVD - fit SOLO su train\n",
    "X_bowsvd_train, X_bowsvd_val, X_bowsvd_test = truncated_svd(X_bow_train, X_bow_val, X_bow_test)\n",
    "X_tfidfsvd_train, X_tfidfsvd_val, X_tfidfsvd_test = truncated_svd(X_tfidf_train, X_tfidf_val, X_tfidf_test)\n",
    "\n",
    "# Le variabili per la Logistic Regression sono ora pronte:\n",
    "# - X_bowsvd_train, X_bowsvd_val, X_bowsvd_test\n",
    "# - X_tfidfsvd_train, X_tfidfsvd_val, X_tfidfsvd_test\n",
    "# - y_train, y_val, y_test (uguali per entrambi)\n",
    "\n",
    "# Alias per compatibilità con il codice esistente\n",
    "y_bowsvd_train = y_train\n",
    "y_bowsvd_val = y_val\n",
    "y_bowsvd_test = y_test\n",
    "y_tfidfsvd_train = y_train\n",
    "y_tfidfsvd_val = y_val\n",
    "y_tfidfsvd_test = y_test"
   ],
   "id": "668419c872eead2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4942 authors\n",
      "Train: 3952, Val: 495, Test: 495\n",
      "BoW vocabulary size: 21207\n",
      "TF-IDF vocabulary size: 21207\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e64c86bf",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a5dd30b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T12:42:49.605623700Z",
     "start_time": "2026-01-18T12:42:41.054861300Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "8f3aabed",
   "metadata": {},
   "source": [
    "### Defining the functions "
   ]
  },
  {
   "cell_type": "code",
   "id": "e0c9d9a0",
   "metadata": {},
   "source": [
    "# uses SelectKBest with chi-square to select the K-most informative features reducing dimensionality.\n",
    "# Returns transformed data and the selector.\n",
    "\n",
    "def select_top_k_features(X_train, y_train, X_val, k=10000):\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "    X_val_sel = selector.transform(X_val)\n",
    "\n",
    "    return X_train_sel, X_val_sel, selector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf2d479f",
   "metadata": {},
   "source": [
    "# It uses RandomOverSampler for balancing the dataset increasing the minority class.\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_bal, y_bal = ros.fit_resample(X, y)\n",
    "    return X, y_bal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99eee255",
   "metadata": {},
   "source": [
    "\n",
    "def train_multinomialNB(X_train, y_train, batch_size=256, alpha=1.0):\n",
    "\n",
    "    #model multinomial Naive Bayes\n",
    "    classifier = MultinomialNB(alpha=alpha, force_alpha=True, fit_prior=True)\n",
    "    \n",
    "    first = True\n",
    "    n = X_train.shape[0]\n",
    "\n",
    "    # generation of batch to reduce the usage of the RAM\n",
    "    for i in range(0, n, batch_size):\n",
    "        end = min(i + batch_size, n)  # last batch must not over the max number of samplesas\n",
    "        X_batch = X_train[i:end]\n",
    "        y_batch = y_train[i:end]\n",
    "\n",
    "        if first:\n",
    "            classifier.partial_fit(X_batch, y_batch, classes=[0,1])\n",
    "            first = False\n",
    "        else:\n",
    "            classifier.partial_fit(X_batch, y_batch)\n",
    "\n",
    "    return classifier\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20900af7",
   "metadata": {},
   "source": [
    "def train_complementNB(X_train, y_train, batch_size=256, alpha=1.0):\n",
    "\n",
    "    classifier = ComplementNB(alpha=alpha)\n",
    "    \n",
    "    # fit the model with partial fitting using batch(to save the RAM)\n",
    "    n = X_train.shape[0]\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "\n",
    "    first = True\n",
    "\n",
    "\n",
    "    # generation of batch to reduce the usage of the RAM\n",
    "    for i in range(0, n, batch_size):\n",
    "        end = min(i + batch_size, n)  # assicura che l'ultimo batch non superi il numero di campioni\n",
    "\n",
    "        batch_idz = indices[i:end]\n",
    "        X_batch = X_train[i:end]\n",
    "        y_batch = y_train[i:end]\n",
    "\n",
    "        if first:\n",
    "            classifier.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
    "            first = False\n",
    "        else:\n",
    "            classifier.partial_fit(X_batch, y_batch)\n",
    "     \n",
    "\n",
    "    return classifier"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4778106b",
   "metadata": {},
   "source": [
    "# generates forecasts for training, validation and the test sets, printing classification reports\n",
    "# and confusion matrix\n",
    "\n",
    "def evaluate_model(classifier, X_train, y_train, X_val, y_val, dataset=\"\"):\n",
    "\n",
    "    # predictions\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_val = classifier.predict(X_val)\n",
    "\n",
    "\n",
    "    print(f\"Info about: {dataset}\")\n",
    "\n",
    "    # evaluating performances\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    print(f\"Classification report validation:\\n{classification_report(y_val,y_pred_val)}\")\n",
    "    print(f\"Classification report training:\\n{classification_report(y_train,y_pred_train)}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a201e42",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Alpha il the Laplace smoothing and it is useful to avoid that a word with count 0 drop to 0 the probability of the relegated class."
   ]
  },
  {
   "cell_type": "code",
   "id": "3529f86e",
   "metadata": {},
   "source": [
    "# searches the best alpha for Naive Bayes and returns the best performing model along the optimal value.\n",
    "\n",
    "def tune_alpha_nb(X_train, y_train, X_val, y_val, type=0):\n",
    "\n",
    "    best_alpha = None\n",
    "    best_f1 = 0\n",
    "    best_clf = None\n",
    "\n",
    "    alpha_list = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "\n",
    "        if type == 0:\n",
    "            classifier = train_multinomialNB(X_train, y_train, batch_size=256, alpha=alpha)\n",
    "        else:\n",
    "            classifier = train_complementNB(X_train, y_train, batch_size=256, alpha=alpha)\n",
    "        y_val_pred = classifier.predict(X_val)\n",
    "        report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "        f1_macro = report['macro avg']['f1-score']\n",
    "\n",
    "        \n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            best_alpha = alpha\n",
    "            best_clf = classifier\n",
    "\n",
    "    print(f\"Best alpha: {best_alpha}, Macro F1: {best_f1:.4f}\")\n",
    "    return best_clf, best_alpha\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c332d052",
   "metadata": {},
   "source": [
    "### Use of Bow\n",
    "Applying the Multinomial Naive Bayes to the BoW."
   ]
  },
  {
   "cell_type": "code",
   "id": "75f3257d",
   "metadata": {},
   "source": [
    "# Evaluating model \n",
    "X_train_, X_val_, selector = select_top_k_features(X_bowquantile_train, y_bowquantile_train,X_bowquantile_val, k=10000)\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_classes(X_train_, y_bowquantile_train)\n",
    "\n",
    "multinomial_nb = train_multinomialNB(X_train_balanced, y_train_balanced, batch_size=256, alpha=1.0)\n",
    "\n",
    "evaluate_model(multinomial_nb, X_train_, y_bowquantile_train, X_val_, y_bowquantile_val, dataset=\"BoW quantile\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8979eef",
   "metadata": {},
   "source": [
    "# evaluating the test set\n",
    "X_bowquantile_test_ = selector.transform(X_bowquantile_test)\n",
    "\n",
    "evaluate_model(\n",
    "    multinomial_nb,\n",
    "    X_train_, y_bowquantile_train, \n",
    "    X_bowquantile_test_, y_bowquantile_test,    # test set\n",
    "    dataset=\"BoW quantile - Test\"\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ed27219",
   "metadata": {},
   "source": [
    "# Tuning aplha\n",
    "\n",
    "# Step 1: selection of top-k\n",
    "X_train_sel, X_val_sel, selector = select_top_k_features(X_bowquantile_train, y_bowquantile_train, X_bowquantile_val, k=10000)\n",
    "\n",
    "# Step 2: oversampling\n",
    "X_train_bal, y_train_bal = balance_classes(X_train_sel, y_bowquantile_train)\n",
    "\n",
    "# Step 3: tuning alpha\n",
    "best_model, best_alpha = tune_alpha_nb(X_train_bal, y_train_bal, X_val_sel, y_bowquantile_val, type=0)\n",
    "\n",
    "# Step 4: valutazione finale\n",
    "evaluate_model(best_model, X_train_sel, y_bowquantile_train, X_val_sel, y_bowquantile_val, dataset=\"BoW Quantile Top-K + Oversampling\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f930345c",
   "metadata": {},
   "source": [
    "### Use of TF-IDF\n",
    "We should use Complement NB to avoid the problem to drop to 0 when a feature is less or equal than 0.\n",
    "MultinomialNB works well only when the features are non negative integers."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b12ea70",
   "metadata": {},
   "source": [
    "# Evaluating model \n",
    "X_train_, X_val_, selector = select_top_k_features(X_tfidfquantile_train, y_tfidfquantile_train,X_tfidfquantile_val, k=10000)\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_classes(X_train_, y_tfidfquantile_train)\n",
    "\n",
    "multinomial_nb = train_complementNB(X_train_balanced, y_train_balanced, batch_size=256, alpha=1.0)\n",
    "\n",
    "evaluate_model(multinomial_nb, X_train_, y_tfidfquantile_train, X_val_, y_tfidfquantile_val, dataset=\"TD-IDF Quantile\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81adb7b1",
   "metadata": {},
   "source": [
    "# evaluating the test set\n",
    "X_tfidfquantile_test_ = selector.transform(X_bowquantile_test)\n",
    "\n",
    "evaluate_model(\n",
    "    multinomial_nb,\n",
    "    X_train_, y_tfidfquantile_train, \n",
    "    X_tfidfquantile_test_, y_tfidfquantile_test,    # test set\n",
    "    dataset=\"TD-IDF quantile - Test\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70a72228",
   "metadata": {},
   "source": [
    "# Top-K + oversampling\n",
    "X_train_tf_topk, X_val_tf_topk, selector_tf = select_top_k_features(X_tfidfquantile_train, y_tfidfquantile_train, X_tfidfquantile_val, k=10000)\n",
    "X_train_tf_bal, y_train_tf_bal = balance_classes(X_train_tf_topk, y_tfidfquantile_train)\n",
    "\n",
    "# Tuning alpha\n",
    "clf_best, best_alpha = tune_alpha_nb(X_train_tf_bal, y_train_tf_bal, X_val_tf_topk, y_tfidfquantile_val, type=1)\n",
    "\n",
    "# Eval finale\n",
    "evaluate_model(clf_best, X_train_tf_topk, y_tfidfquantile_train, X_val_tf_topk, y_tfidfquantile_val, dataset=\"TF-IDF + ComplementNB\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d208c121",
   "metadata": {},
   "source": [
    "# Model Results Commentary (Training, Validation, Test, Tuning)\n",
    "\n",
    "## 1. BoW Quantile + MultinomialNB\n",
    "- Training accuracy (0.87) and validation accuracy (0.83) are close, showing minimal overfitting.\n",
    "- Class 1 (minority) performance is lower but acceptable after oversampling.\n",
    "- The model generalizes reasonably well, with a normal gap for Naive Bayes.\n",
    "\n",
    "**Conclusion:**  \n",
    "Stable model with slight but acceptable overfitting and moderate difficulty on the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. BoW Quantile + ComplementNB\n",
    "- Performance is similar to MultinomialNB, with comparable validation macro-F1.\n",
    "- Class 1 recall remains lower, but overall consistency is good between training and validation.\n",
    "- Overfitting is minimal and well-controlled.\n",
    "\n",
    "**Conclusion:**  \n",
    "Reliable generalization with no critical overfitting issues.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. TF-IDF Quantile + ComplementNB (Before Tuning)\n",
    "- Very high training accuracy (0.93) but validation accuracy drops to ~0.79.\n",
    "- Minority class performance collapses (validation recall ≈ 0.43, test recall ≈ 0.32).\n",
    "- This strong gap indicates clear overfitting, amplified by high-dimensional TF-IDF features.\n",
    "\n",
    "**Conclusion:**  \n",
    "This configuration overfits heavily and generalizes poorly, especially for the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. TF-IDF Quantile + ComplementNB (After Alpha Tuning)\n",
    "- Best alpha identified: 0.1.\n",
    "- Validation performance improves and becomes more balanced (macro-F1 ≈ 0.75).\n",
    "- Training accuracy remains very high (0.95), while validation stabilizes around 0.80.\n",
    "- Overfitting is reduced but still present due to the persistent performance gap.\n",
    "\n",
    "**Conclusion:**  \n",
    "Tuning improves generalization, but TF-IDF + ComplementNB continues to show moderate overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Summary\n",
    "- BoW-based models are more stable and show better generalization with minimal overfitting.\n",
    "- TF-IDF models tend to overfit significantly, especially without smoothing.\n",
    "- The minority class is consistently harder to model across all pipelines.\n",
    "- Reducing the feature count or applying stronger regularization could further improve performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
