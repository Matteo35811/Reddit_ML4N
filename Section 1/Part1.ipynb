{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec76bf60",
   "metadata": {},
   "source": [
    "# Part 1.3: Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2684a",
   "metadata": {},
   "source": [
    "## Data Loading:\n",
    "How many different attribute values do you observe in each feature? (e.g. how many\n",
    "subreddits are there?) Is there any missing or duplicated data? (Referring to textual\n",
    "features)\n",
    "b. How does the empirical distribution of the number of characters in each comment look\n",
    "like? How is the distribution of the number of comments per author? Is the supervised\n",
    "dataset balanced between male and female? Are there only comments in English? Hint: use\n",
    "the library langdetect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7b9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:17.139304Z",
     "start_time": "2025-11-20T16:19:49.050597Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8be56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:19.121601Z",
     "start_time": "2025-11-20T16:20:17.150567Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1c33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:20.854289Z",
     "start_time": "2025-11-20T16:20:19.162078Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca154de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:21.004502Z",
     "start_time": "2025-11-20T16:20:20.893622Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3c00c",
   "metadata": {},
   "source": [
    "## Data Exploration:\n",
    "a. How many different attribute values do you observe in each feature? (e.g. how many\n",
    "subreddits are there?) Is there any missing or duplicated data? (Referring to textual\n",
    "features)\n",
    "b. How does the empirical distribution of the number of characters in each comment look\n",
    "like? How is the distribution of the number of comments per author? Is the supervised\n",
    "dataset balanced between male and female? Are there only comments in English? Hint: use\n",
    "the library langdetect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading the supervised data\n",
    "df = pd.read_csv('../data/data_supervised.csv') #COMMENT\n",
    "#print(df.head(3))\n",
    "\n",
    "print(f\"Dataframe size: {df.shape}\")\n",
    "m = df.shape\n",
    "\n",
    "# count differents subreddit topics and authors\n",
    "n_distinc_authors = len(pd.unique(df['author']))\n",
    "print(f\"There are {n_distinc_authors} distinct authors\")\n",
    "\n",
    "# count distinct subreddit (topics)\n",
    "n_distinct_subreddit = len(pd.unique(df['subreddit']))\n",
    "print(f\"There are {n_distinct_subreddit} distinct subreddit\")\n",
    "\n",
    "# count distinct body (comments)\n",
    "n_distinct_body = len(pd.unique(df['body']))\n",
    "print(f\"There are {n_distinct_body} distinct body\")\n",
    "\n",
    "# checking missing values\n",
    "print(df.isna().sum())\n",
    "\n",
    "# checking empty stirng\n",
    "print(\"Empty bodies: \",(df['body'].str.strip() == \"\").sum())\n",
    "\n",
    "# checking duplicate values\n",
    "n_duplicate = df.duplicated().sum()\n",
    "print(f\"Duplicate values {n_duplicate}\")\n",
    "\n",
    "# count rows with same author and same body\n",
    "print(df['body'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the number of character for each comment\n",
    "df['char_len'] = df['body'].str.len()\n",
    "\n",
    "# describing the empirical distribution\n",
    "print(df['char_len'].describe())\n",
    "\n",
    "# plot the histogram\n",
    "sns.histplot(df['char_len'], bins=50)\n",
    "plt.title(\"Distribution of Comment Length (characters)\")\n",
    "plt.xlim(0, 14271) #CCC\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments per author\n",
    "comments_per_author = df['author'].value_counts()\n",
    "print(comments_per_author.describe())\n",
    "\n",
    "plt.xlabel(\"Comments\")\n",
    "plt.ylabel(\"Users\")\n",
    "\n",
    "# plotting\n",
    "sns.histplot(comments_per_author, bins=50, log_scale=True)\n",
    "plt.title(\"Distribution of Comments per Author (log scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4694df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the target_supervised\n",
    "target = pd.read_csv('../data/target_supervised.csv')\n",
    "\n",
    "target['gender'].value_counts()\n",
    "target['gender'].value_counts(normalize=True)\n",
    "\n",
    "sns.countplot(data=target, x='gender')\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#CCC percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbdeab",
   "metadata": {},
   "source": [
    "# Data Cleaning and Text Standardization.\n",
    "\n",
    "a. Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case).\n",
    "If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n",
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry\n",
    "very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in\n",
    "reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK\n",
    "or spaCy for tokenization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import html\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29218c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervised   = pd.read_csv(\"../data/data_supervised.csv\")\n",
    "df_unsupervised = pd.read_csv(\"../data/data_unsupervised.csv\")\n",
    "df_target       = pd.read_csv(\"../data/target_supervised.csv\")\n",
    "\n",
    "print(df_supervised.shape, df_unsupervised.shape, df_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212d9bf",
   "metadata": {},
   "source": [
    "Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case). If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e86078",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pattern = r'https?://\\S+|www\\.\\S+|r/\\w+|u/\\w+'\n",
    "\n",
    "df_supervised['body_normalized'] = (\n",
    "    df_supervised['body']\n",
    "    .fillna('')                                     # Gestisce i NaN\n",
    "    .astype(str)                                    # Assicura formato stringa\n",
    "    .str.lower()                                    # Case normalization (Punto a.)\n",
    "    .apply(html.unescape)                           # Decodifica HTML (es. &amp; -> &)\n",
    "    .str.replace(remove_pattern, ' ', regex=True) # Rimuove URL, r/, u/\n",
    "    .str.replace(r'\\s+', ' ', regex=True)           # Rimuove doppi spazi\n",
    "    .str.strip()                                    # Pulisce spazi inizio/fine\n",
    ")\n",
    "\n",
    "df_unsupervised['body_normalized'] = (\n",
    "    df_unsupervised['body']\n",
    "    .fillna('')\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .apply(html.unescape)\n",
    "    .str.replace(remove_pattern, ' ', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKK!!!\n",
    "df_supervised[[\"body\", 'body_normalized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f3d5f",
   "metadata": {},
   "source": [
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK or spaCy for tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74011b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_full(text_series, batch_size=2000):\n",
    "    clean_texts = []\n",
    "\n",
    "    total_docs = len(text_series)\n",
    "\n",
    "    # tqdm show the process bar\n",
    "    for doc in tqdm(nlp.pipe(text_series, batch_size=batch_size), total=total_docs, desc=\"Processing\"):\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # 1. Filtering Stop Words e punctation (b)\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # 2. Take the lemma using spaCy (c)\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "\n",
    "    return clean_texts\n",
    "\n",
    "print(\"Elaboration of SUPERVISED dataset (smaller)...\")\n",
    "df_supervised['body_clean'] = process_text_full(df_supervised['body_normalized'].astype(str))\n",
    "\n",
    "df_supervised.to_csv(\"./clean_supervised.csv\", index=False)\n",
    "\n",
    "print(\"Elaboration of UNSUPERVISED  dataset (bigger)...\")\n",
    "df_unsupervised['body_clean'] = process_text_full(df_unsupervised['body_normalized'].astype(str))\n",
    "\n",
    "df_unsupervised.to_csv(\"./clean_unsupervised.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3c0a8",
   "metadata": {},
   "source": [
    "# 1.3 Text Vectorization.\n",
    "\n",
    "A. Only for the supervised task (data_supervised.csv): Group and join all comments of the\n",
    "same author, creating a “new” dataset to be used for the supervised task (Section 2).\n",
    "\n",
    "B: As ML algorithms struggle to handle directly the raw textual data. You are required to\n",
    "convert the text into numerical representations (vectors) through Bag of Words (BoW).\n",
    "\n",
    "C: Another way to assign a vector representation to a word is to associate the TF-IDF\n",
    "representation (Term Frequency-Inverse Document Frequency) to each user/comment.\n",
    "Can you observe and explain the differences between the numerical representations\n",
    "generated by BoW and TF-IDF?\n",
    "\n",
    "\n",
    "A Wrap up section at the bottom describing which files are created and where by this notebook is at the very end of the notebook. To change the input file instead view the very first cell of the notebook. Originally meant for only the supervised dataset as described by the requirements.\n",
    "\n",
    "For problems contact Matteo Sottocornola on Telegram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891fbc2",
   "metadata": {},
   "source": [
    "## Part 1 of 1.3\n",
    "\n",
    "Only for the supervised task (data_supervised.csv): Group and join all comments of the\n",
    "same author, creating a “new” dataset to be used for the supervised task (Section 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO capire se aggiungere aggregated subreddit/created_utc\n",
    "\n",
    "# import as panda dataframe.\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./clean_supervised.csv\") #In principio da usare solo su clean_supervised.\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the two unneded columns inside clean_supervised\n",
    "#Rename the cleaned body to just body cause I prefer that way.\n",
    "df = df.drop(columns=['body','body_normalized'])\n",
    "df = df.rename(columns={'body_clean':'body'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non sicuro se anche questi andrano tenuti/concatenati come body quindi drop per ora.\n",
    "df_text_only = df.drop(columns=['created_utc','subreddit'])\n",
    "print(df_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_text_only.shape)\n",
    "df_text_only = df_text_only.dropna(subset=['body']) #perdiamo un 6000 su 296,000 posts.\n",
    "print(df_text_only.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_text_only.groupby('author')['body'].apply(\" \".join).reset_index()\n",
    "print(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc35469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick sanity check.\n",
    "i=0 #select an index, and as such a user.\n",
    "user = df.iloc[i,0]\n",
    "print(\"user: \", user,\" posted this: \", df.iloc[i, 3])\n",
    "\n",
    "print(df.groupby(\"author\").size().loc[user] )\n",
    "#df.groupby(\"author\").count()['author'=df.iloc[0, 0]]\n",
    "\n",
    "row = df_grouped[df_grouped[\"author\"] == user] #user\n",
    "print(row.iloc[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d2bd4",
   "metadata": {},
   "source": [
    "## Part 2 of 1.3\n",
    "B: As ML algorithms struggle to handle directly the raw textual data. You are required to\n",
    "convert the text into numerical representations (vectors) through Bag of Words (BoW).\n",
    "\n",
    "Bag of Words (BoW) is a technique widely used to transform textual data into machine-readable format, specifically numerical values, without considering grammar and word order.\n",
    "\n",
    "We will be counting the occurence of every word in the vocabulary we use. Where the word was and it's actual structure is lost. Basically you are adding a new column for each word that is in our dataset and adding the number of times it was used for each row.\n",
    "\n",
    "Note that executing 1.2 to remove stop words first is heavily recomended to reduce the number of words and hence attributes we get with BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful example of BoW\n",
    "#https://www.datacamp.com/tutorial/python-bag-of-words-model?dc_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "#df_grouped = df_grouped[:400] #remove after 1.2 available, done to reduce complexity for now.\n",
    "\n",
    "# Function to preprocess and tokenize\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize: split the text into words\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each text individually first.\n",
    "processed_corpus = []\n",
    "for sentence in df_grouped.iloc[:, 1]:\n",
    "    processed_corpus.append(preprocess(sentence))\n",
    "\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dea5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the individual tokenized texts and create a single vocabulary.\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "# Build the vocabulary\n",
    "for sentence in processed_corpus:\n",
    "    vocabulary.update(sentence)\n",
    "\n",
    "# Convert to a sorted list\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "#If it doesn't look correct it's cause it tokenizes numbers too and resorts them in it's own logic\n",
    "#scroll enough and you find the words like you should.\n",
    "\n",
    "print( len(vocabulary) ) #How many unique words and as such how many features we end up adding.\n",
    "#19186 first time with only first 100, too much. Need the preprocessing of 1.1 and 1.2 to reduce.\n",
    "#I'd also consider removing numbers which are being classed as words.\n",
    "#Could also pre process by rounding numbers to nearest multiple of ten to reduce the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f17b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w:i for i,w in enumerate(vocabulary)} #Create a dictionary so future lookups are O(1)\n",
    "\n",
    "def create_bow_vector(sentence, vocab):\n",
    "    vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "    for word in sentence:\n",
    "        idx = word2idx.get(word)  # Find the index of the word in the vocabulary using dictionary.\n",
    "        if idx is not None:\n",
    "            vector[idx] += 1  # Increment the count at that index\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Create BoW vector for each sentence in the processed corpus\n",
    "bow_vectors = [create_bow_vector(sentence, vocabulary) for sentence in processed_corpus]\n",
    "print(\"Bag of Words Vectors:\")\n",
    "for vector in bow_vectors[:100]:\n",
    "    print(vector)\n",
    "\n",
    "#Lento perche troppe parole nel vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa840755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "#Double check if plausible used a certain number of times the same word.\n",
    "row = df_grouped[df_grouped[\"author\"] == df_grouped.iloc[17,0]]\n",
    "print(row.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ced30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop no longer needed body.\n",
    "df_grouped_bow = df_grouped.drop(columns = ['body'])\n",
    "\n",
    "#Add the BoW instead.\n",
    "df_grouped_bow[\"bow\"] = bow_vectors\n",
    "\n",
    "#Print a few to see\n",
    "print(df_grouped_bow[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e94561",
   "metadata": {},
   "source": [
    "# Part 3 of 1.3 - TF-IDF\n",
    "\n",
    "Another way to assign a vector representation to a word is to associate the TF-IDF\n",
    "representation (Term Frequency-Inverse Document Frequency) to each user/comment.\n",
    "Can you observe and explain the differences between the numerical representations\n",
    "generated by BoW and TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d50409",
   "metadata": {},
   "source": [
    "For TF-IDF it is necessary to create the vocabulary of all the distinct words and then for each word performing the following calculation which requires computing these two first... \n",
    "\n",
    "![Fig1.png](Fig1.png)\n",
    "\n",
    "\n",
    "In practice for every word A and each user we calculate two metrics, one is what percentage of overall users used word A (log of this number). And the other is what percentage of the words used by that specific user correspond to A. The index is then the multiple of these two.\n",
    "\n",
    "It's meant to provide a measure of how important each word is to that text, but corrected to cancel out words that are just common in general and not common to this specific text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We reuse the vocabulary computed for the previous section so execute that first.\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Count in how many documents each word appears\n",
    "df_counter = Counter()\n",
    "\n",
    "for sentence in processed_corpus:\n",
    "    unique_words = set(sentence)     # ensure each word counted only once per doc\n",
    "    df_counter.update(unique_words)\n",
    "\n",
    "N = len(processed_corpus)            # number of documents\n",
    "\n",
    "idf = [math.log(N / df_counter.get(word, 1)) for word in vocabulary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(idf))\n",
    "print(len(idf))\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aec198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the idf factor for each word in the vocabulary.\n",
    "#idf = []\n",
    "#import math\n",
    "\n",
    "#for word in vocabulary:\n",
    "#    count = 0\n",
    "#    for sentence in processed_corpus:\n",
    "#       if word in sentence:\n",
    "#           count += 1\n",
    "#    count = (df_grouped.shape[0]) / (count) #Aggiunta 1 necessaria per evitare eventuali divisioni per zero.\n",
    "##    count = math.log(count) \n",
    " #   idf.append(count)\n",
    "\n",
    "#print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f35414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(3000):\n",
    "#    print(i, vocabulary[i], \" \", idf[i])\\\n",
    "    \n",
    "#LE PAROLE MENO COMUNI HANNO VALORI PIU ALTI.\n",
    "#print(\"parola comune: \", vocabulary[2996], \" idf: \", idf[2996])\n",
    "#print(\"parola rara: \", vocabulary[2781], \" idf: \", idf[2781])\n",
    "#print(\"Le parole le usate dai diversi users hanno valori piu bassi, quanto sono stati usati dal singolo user non influisce.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to compute the second metric TF and directly the TF-IDF\n",
    "\n",
    "#word2idx\n",
    "\n",
    "def create_TF_IDF_vector(sentence, vocab, idf):\n",
    "    vector = [0] * len(vocab)  # If not present in user's comment then TF-IDF is 0.\n",
    "    for word in sentence:\n",
    "        idx = word2idx.get(word)  # Find the index of the word in the vocabulary\n",
    "        if idx is not None:\n",
    "            vector[idx] += 1  # Increment the count at that index\n",
    "    \n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        vector[i] = vector[i] / len(sentence)\n",
    "        vector[i] = vector[i] * idf[i]\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca927e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vector for each sentence in the processed corpus\n",
    "TFIDF_vectors = [create_TF_IDF_vector(sentence, vocabulary, idf) for sentence in processed_corpus]\n",
    "print(\"TF-IDF Vectors:\")\n",
    "for vector in TFIDF_vectors[:100]:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug - checks\n",
    "for sentence in processed_corpus:\n",
    "    print(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb56807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug - checks\n",
    "ind = 0\n",
    "for el in TFIDF_vectors[1]:\n",
    "    if el != 0.0:\n",
    "        print(el, ind)\n",
    "    ind = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug - checks\n",
    "print(idf[8513]*0.5, idf[18680]*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    i = i+20 #solo per non guardare sempre gli stessi.\n",
    "    print(\"index: \", i, \"TF-IDF: \",TFIDF_vectors[i])\n",
    "    print(\"index: \", i, \"bow_words: \",bow_vectors[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for el in TFIDF_vectors[1]:\n",
    "    if el != 0.0:\n",
    "        print(el, ind)\n",
    "        print(\"index: \", ind, \" TF-IDF: \", el, \" for word: \", vocabulary[ind])\n",
    "    ind = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22168ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for el in bow_vectors[1]:\n",
    "    if el != 0.0:\n",
    "        print(el, ind)\n",
    "        print(\"index: \", ind, \" TF-IDF: \", el, \" for word: \", vocabulary[ind])\n",
    "    ind = ind + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f611d5",
   "metadata": {},
   "source": [
    "The main difference is visible quite clearly by printing the previous two cells for user with index 1. As we can see that user ever only typed five words, presumably in the same comment which we can divine from the word to have been 'really just read the faq'\n",
    "\n",
    "With BoW we replace each word simply with the count of it, and hence get a vector of only zeroes for all the other words in vocabulary and 1 for these five.\n",
    "\n",
    "On the other hand TF-IDF does a more complex computation where 'the' also considers the count in the sentence (1) but also how common the word is among all the users and seeing that it's quite common it gets a much smaller value than the others. This is meant to give us an idea that the word 'the' probably doesn't carry a lot of significance as it's more common.\n",
    "\n",
    "Which is best for our task seems disputable, for instance if there was a hypothetical word predominantly used by male redditers TF-IDF would eroneously assign it a small value as a large number of the users would have used it. On the other hand such a magical classifying word is unlikely to be present and intuitively it is preferable to give lesser weight to overly common words unlikely to carry much significance.\n",
    "\n",
    "The solution naturally is to try our models with both and check which is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f8fed",
   "metadata": {},
   "source": [
    "# Wrap up - Writing the datasets\n",
    "\n",
    "Naturally at this point I must save the two stored representations with a name that allows us to distinguish them at a glance yet remember they came from the output of 1.3\n",
    "\n",
    "I will go with\n",
    "supervised-1.3-BoW\n",
    "supervised-1.3-TF-IDF\n",
    "\n",
    "I am uncertain whether the vocabulary is also needed for further sections so I will also create\n",
    "1.3-vocab.csv\n",
    "\n",
    "which I will save to the data folder but also add to the .gitignore.\n",
    "If you need the files re-execute 1.3 locally.\n",
    "Remember you can change which file is used as input at the top.\n",
    "\n",
    "For questions/problems it was Matteo Sottocornola who did this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocabulary)\n",
    "vocab_df = pd.DataFrame(vocabulary, columns=[\"word\"])\n",
    "#print(vocab_df)   #presenza emoji normale, presenti nel testo iniziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868bc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e598954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_grouped.drop(columns=['body'])\n",
    "usernames = df_grouped[\"author\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6145c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TFIDF_vectors))\n",
    "print(type(TFIDF_vectors))\n",
    "print(type(usernames))\n",
    "print(usernames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_usernames = [\n",
    "    [usernames.iloc[i]] + TFIDF_vectors[i]\n",
    "    for i in range(len(TFIDF_vectors))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eaea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_df = pd.DataFrame(TFIDF_vectors, columns=vocabulary)\n",
    "#bow_df = pd.DataFrame(bow_vectors, columns=vocabulary)\n",
    "#tfidf_df = pd.merge(usernames, tfidf_df, left_index=True, right_index=True)\n",
    "#bow_df = pd.merge(usernames, bow_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08023271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf_usernames))\n",
    "\n",
    "for row in tfidf_usernames[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df.to_csv(\"./1.3-vocab.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"supervised-1.3-TF-IDF.csv\",\"w\",newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tfidf_usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb318308",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_usernames = [\n",
    "    [usernames.iloc[i]] + bow_vectors[i]\n",
    "    for i in range(len(bow_vectors))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"supervised-1.3-BoW.csv\",\"w\",newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tfidf_usernames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
