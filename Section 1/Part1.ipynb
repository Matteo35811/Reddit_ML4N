{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec76bf60",
   "metadata": {},
   "source": [
    "# Section 1: Data exploration and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2684a",
   "metadata": {},
   "source": [
    "## Requirements and utilities\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "37b7b9ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:17.139304Z",
     "start_time": "2025-11-20T16:19:49.050597Z"
    }
   },
   "outputs": [],

   "source": [
    "!pip install pandas scikit-learn nltk "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "4c8be56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:19.121601Z",
     "start_time": "2025-11-20T16:20:17.150567Z"
    }
   },
   "outputs": [],

   "source": [
    "!pip install langdetect"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "40e1c33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:20:20.854289Z",
     "start_time": "2025-11-20T16:20:19.162078Z"
    }
   },
   "outputs": [],

   "source": [
    "!pip install seaborn matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "ca154de4",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "684b7704",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfc3c00c",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "*a. How many different attribute values do you observe in each feature? (e.g. how many\n",
    "subreddits are there?) Is there any missing or duplicated data? (Referring to textual\n",
    "features)*\n",
    "\n",
    "*b. How does the empirical distribution of the number of characters in each comment look\n",
    "like? How is the distribution of the number of comments per author? Is the supervised\n",
    "dataset balanced between male and female? Are there only comments in English? Hint: use\n",
    "the library langdetect.*"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "464b8401",
   "metadata": {},
   "source": [
    "\n",
    "# loading the supervised data\n",
    "df = pd.read_csv('../data/data_supervised.csv') \n",
    "\n",
    "\n",
    "print(f\"Dataframe size: {df.shape}\")\n",
    "m = df.shape\n",
    "\n",
    "# count differents subreddit topics and authors\n",
    "n_distinc_authors = len(pd.unique(df['author']))\n",
    "print(f\"There are {n_distinc_authors} distinct authors\")\n",
    "\n",
    "# count distinct subreddit (topics)\n",
    "n_distinct_subreddit = len(pd.unique(df['subreddit']))\n",
    "print(f\"There are {n_distinct_subreddit} distinct subreddit\")\n",
    "\n",
    "# count distinct body (comments)\n",
    "n_distinct_body = len(pd.unique(df['body']))\n",
    "print(f\"There are {n_distinct_body} distinct body\")\n",
    "\n",
    "# checking missing values\n",
    "print(\"\\nNull value count for each field\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# checking empty stirng\n",
    "print(\"\\nEmpty bodies: \",(df['body'].str.strip() == \"\").sum())\n",
    "\n",
    "# checking duplicate values\n",
    "n_duplicate = df.duplicated().sum()\n",
    "print(f\"Duplicate rows {n_duplicate}\")\n",
    "\n",
    "# count rows with same body.\n",
    "print(\"Number of identical bodies: \", df['body'].duplicated().sum())\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "429f7dab",
   "metadata": {},
   "outputs": [],

   "source": [
    "# collect the number of character for each comment\n",
    "df['char_len'] = df['body'].str.len()\n",
    "\n",
    "# describing the empirical distribution\n",
    "print(\"Body char length statistics:\")\n",
    "print(df['char_len'].describe())\n",
    "\n",
    "# plot the histogram\n",
    "sns.histplot(df['char_len'], bins=50)\n",
    "plt.title(\"Distribution of Comment Length (characters)\")\n",
    "plt.xlim(0, 14271)\n",
    "plt.show()\n",
    "\n",
    "#TODO Scegliere se tenere entrambi o solo uno dei due plot.\n",
    "#99-percentile plot\n",
    "per_len_plot = df['char_len']\n",
    "per_len_plot = per_len_plot[per_len_plot <= per_len_plot.quantile(0.99)]\n",
    "sns.histplot(per_len_plot, bins=50)\n",
    "plt.title(\"Distribution of Comment Length (characters) - 99th percentile cut\")\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(df['char_len'])\n",
    "plt.xlabel(\"Comments per Author\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"Empirical Cumulative Distribution of Comments per Author\")\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(per_len_plot)\n",
    "plt.xlabel(\"Comments per Author\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"ECDF of Comments per Author (99 percentile cut)\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "7e46b47c",
   "metadata": {},
   "outputs": [],

   "source": [
    "# comments per author\n",
    "comments_per_author = df['author'].value_counts()\n",
    "print(\"Statistics of number of comments per author\")\n",
    "print(comments_per_author.describe())\n",
    "\n",
    "plt.xlabel(\"Comments\")\n",
    "plt.ylabel(\"Users\")\n",
    "\n",
    "# plotting\n",
    "sns.histplot(comments_per_author, bins=50)\n",
    "plt.title(\"Distribution of Comments per Author\")\n",
    "plt.show()\n",
    "\n",
    "#99-percentile plot\n",
    "percentile_comments_per_author = comments_per_author[comments_per_author <= comments_per_author.quantile(0.99)]\n",
    "sns.histplot(percentile_comments_per_author, bins=50)\n",
    "plt.title(\"Distribution of Comments per author(99th percentile cut)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.ecdfplot(comments_per_author)\n",
    "plt.xlabel(\"Comments per Author\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"Empirical Cumulative Distribution of Comments per Author\")\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(percentile_comments_per_author)\n",
    "plt.xlabel(\"Comments per Author\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"ECDF of Comments per Author (99 percentile cut)\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "7e4694df",
   "metadata": {},
   "outputs": [],

   "source": [
    "# reading the target_supervised\n",
    "target = pd.read_csv('../data/target_supervised.csv')\n",
    "\n",
    "sns.countplot(data=target, x='gender')\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.show()\n",
    "\n",
    "counts = target['gender'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.axis('equal')  # keeps the pie circular\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "4a38fb28",
   "metadata": {},
   "outputs": [],

   "source": [
    "!pip install fasttext"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LANGDETECT PARWT",
   "id": "64e811446b88ddbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deterministic\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_lang_safe(text:  str) -> str:\n",
    "    try:\n",
    "        if not isinstance(text, str) or len(text.strip()) < 3:\n",
    "            return \"short_text\"  # Rinomina per chiarezza!\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"detection_failed\"\n",
    "\n",
    "# Progress bar\n",
    "print(\"Loading...\")\n",
    "tqdm.pandas(desc=\"Detecting language\")\n",
    "df['lang'] = df['body'].fillna('').astype(str).str.strip().progress_apply(detect_lang_safe)\n",
    "\n",
    "# Stats\n",
    "print(\"\\nDistribution\")\n",
    "print(df['lang'].value_counts(dropna=False))\n",
    "pct_en = (df['lang'] == 'en').mean() * 100\n",
    "print(f\"\\nEnglish comments percentage:  {pct_en:.1f}%\")"
   ],
   "id": "a593d4f87eca0758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LANGUAGE TABLE\n",
    "| LANGUAGE         | COUNT       |\n",
    "|:-----------------|:------------|\n",
    "| **en** (Englsih) | **269.687** |\n",
    "| de               | 2.233       |\n",
    "| af               | 2.041       |\n",
    "| fr               | 1.954       |\n",
    "| cy               | 1.720       |\n",
    "| so               | 1.715       |\n",
    "| nl               | 1.629       |\n",
    "| *unknown*        | 1.473       |\n",
    "| tl               | 1.193       |\n",
    "| da               | 1.188       |\n",
    "| no               | 1.149       |\n",
    "| id               | 1.016       |\n",
    "| it               | 911         |\n",
    "| sv               | 765         |\n",
    "| pt               | 752         |\n",
    "| ro               | 694         |\n",
    "| et               | 691         |\n",
    "| ca               | 649         |\n",
    "| es               | 639         |\n",
    "| tr               | 608         |\n",
    "| sw               | 516         |\n",
    "| pl               | 512         |\n",
    "| vi               | 417         |\n",
    "| fi               | 306         |\n",
    "| sq               | 277         |\n",
    "| sl               | 236         |\n",
    "| hr               | 221         |\n",
    "| sk               | 199         |\n",
    "| cs               | 192         |\n",
    "| hu               | 187         |\n",
    "| lt               | 121         |\n",
    "| lv               | 82          |\n",
    "| ja               | 24          |\n",
    "| kn               | 20          |\n",
    "| ko               | 7           |\n",
    "| he               | 5           |\n",
    "| ru               | 4           |\n",
    "| el               | 3           |\n",
    "| hi               | 2           |\n",
    "| zh-cn            | 2           |\n",
    "| th               | 1           |\n",
    "| zh-tw            | 1           |\n",
    "| **TOTAL**        | **296.042** |\n",
    "## NOTE\n",
    "In english there are only 269687 words (91% circa)\n"
   ],
   "id": "a3790619383fc3ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Graphics\n",
    "lang_counts = df['lang'].value_counts()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=lang_counts.index[:15], y=lang_counts.values[: 15], palette=\"viridis\")\n",
    "plt.title(\"Top 15 Languages\", fontsize=15)\n",
    "plt.ylabel(\"Number of comments (Log Scale)\", fontsize=12)\n",
    "plt.xlabel(\"Language\", fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Filtering - ONLY ENGLISH (excluding unknown)\n",
    "print(f\"\\nBefore Filtering:  {len(df)}\")\n",
    "df_filtered = df[df['lang']. isin(['en', 'short_text'])].copy()\n",
    "df_filtered = df_filtered.drop(columns=['lang'])\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Stats\n",
    "n_en = (df['lang'] == 'en').sum()\n",
    "n_short = (df['lang'] == 'short_text').sum()\n",
    "n_failed = (df['lang'] == 'detection_failed').sum()\n",
    "n_other = len(df) - n_en - n_short - n_failed\n",
    "n_removed = len(df) - len(df_filtered)\n",
    "\n",
    "print(f\"English comments kept:  {n_en}\")\n",
    "print(f\"Short text kept (assumed English): {n_short}\")\n",
    "print(f\"Detection failed removed: {n_failed}\")\n",
    "print(f\"Other languages removed: {n_other}\")\n",
    "print(f\"After filtering (only English): {len(df_filtered)} comments\")\n",
    "print(f\"Total removed:  {n_removed} ({n_removed/len(df)*100:.1f}%)\")\n",
    "\n",
    "# OVERWRITE\n",
    "df = df_filtered.copy()\n",
    "print(f\"\\n'df' now contains {len(df)} comments (English + short text)\")\n"
   ],
   "id": "fc01ddc717cd7554",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68cbdeab",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Text Standardization.\n",
    "\n",
    "*a. Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case).If necessary, clean the comment text (e.g. URLs, subreddit refs, …).*\n",
    "\n",
    "*b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry very little information. Take care of these kinds of words.*\n",
    "\n",
    "*c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in\n",
    "reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK\n",
    "or spaCy for tokenization).*\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "9c6a1690",
   "metadata": {},
   "outputs": [],

   "source": [
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "efa6ee74",
   "metadata": {},
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import html\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "from langdetect import detect\n",
    "import os\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "29218c56",
   "metadata": {},
   "source": [
    "df_supervised   = pd.read_csv(\"../data/data_supervised.csv\")\n",
    "df_unsupervised = pd.read_csv(\"../data/data_unsupervised.csv\")\n",
    "df_target       = pd.read_csv(\"../data/target_supervised.csv\")\n",
    "\n",
    "print(df_supervised.shape, df_unsupervised.shape, df_target.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d212d9bf",
   "metadata": {},
   "source": [
    "*Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case). If necessary, clean the comment text (e.g. URLs, subreddit refs, …).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "78e86078",
   "metadata": {},
   "source": [
    "remove_pattern = r'https?://\\S+|www\\.\\S+|r/\\w+|u/\\w+'\n",
    "\n",
    "df_supervised['body_normalized'] = (\n",
    "    df_supervised['body']\n",
    "    .fillna('')                                     # To handling NaN\n",
    "    .astype(str)                                    # Ensure string format\n",
    "    .str.lower()                                    # Case normalization (Punto a.)\n",
    "    .apply(html.unescape)                           # Decoding HTML (es. &amp; -> &)\n",
    "    .str.replace(remove_pattern, ' ', regex=True)   # Removing URL, r/, u/\n",
    "    .str.replace(r'\\s+', ' ', regex=True)           # Removing double blank spaces\n",
    "    .str.strip()                                    # Strip\n",
    ")\n",
    "\n",
    "df_unsupervised['body_normalized'] = (\n",
    "    df_unsupervised['body']\n",
    "    .fillna('')\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .apply(html.unescape)\n",
    "    .str.replace(remove_pattern, ' ', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "67e7daab",
   "metadata": {},
   "source": [
    "print(df_supervised[[\"body\", 'body_normalized']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "128f3d5f",
   "metadata": {},
   "source": [
    "*b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry very little information. Take care of these kinds of words.*\n",
    "\n",
    "*c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK or spaCy for tokenization).*"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "74011b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T11:35:11.121913Z",
     "start_time": "2025-12-11T11:35:01.238128Z"
    }
   },
   "outputs": [],

   "source": [
    "def process_text_full(text_series, batch_size=2000):\n",
    "    clean_texts = []\n",
    "\n",
    "    total_docs = len(text_series)\n",
    "\n",
    "    # tqdm show the process bar\n",
    "    for doc in tqdm(nlp.pipe(text_series, batch_size=batch_size), total=total_docs, desc=\"Processing\"):\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # 1. Filtering Stop Words e punctation (b)\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # 2. Take the lemma using spaCy (c)\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "\n",
    "    return clean_texts\n",
    "\n",
    "\n",
    "if os.path.exists(\"./clean_supervised.csv\") and os.path.exists(\"./clean_supervised.csv\"):\n",
    "    print(\"File found! Loading...\")\n",
    "    df_unsupervised = pd.read_csv(\"./clean_unsupervised.csv\")\n",
    "    df_supervised = pd.read_csv(\"./clean_supervised.csv\")\n",
    "else:\n",
    "    print(\"Elaboration of SUPERVISED dataset (smaller)...\")\n",
    "    df_supervised['body_clean'] = process_text_full(df_supervised['body_normalized'].astype(str))\n",
    "    df_supervised.to_csv(\"./clean_supervised.csv\", index=False)\n",
    "\n",
    "    print(\"Elaboration of UNSUPERVISED  dataset (bigger)...\")\n",
    "    df_unsupervised['body_clean'] = process_text_full(df_unsupervised['body_normalized'].astype(str))\n",
    "    df_unsupervised.to_csv(\"./clean_unsupervised.csv\", index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11bb92be790ddf18",
   "metadata": {},
   "source": [
    "### Plotting of stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3e2c3e76fd15a41",
   "metadata": {},
   "source": [
    "# Calcoliamo il numero di token (parole)\n",
    "df_supervised['n_words_original'] = df_supervised['body'].astype(str).apply(lambda x: len(x.split()))\n",
    "df_supervised['n_words_normalized'] = df_supervised['body_normalized'].astype(str).apply(lambda x: len(x.split()))\n",
    "df_supervised['n_words_clean'] = df_supervised['body_clean'].astype(str).apply(lambda x: len(x.split()))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eecfa00b03dcc7",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.kdeplot(df_supervised['n_words_normalized'], fill=True, label='First Processing (Normalized)', color='salmon', alpha=0.5)\n",
    "sns.kdeplot(df_supervised['n_words_clean'], fill=True, label='Second Processing (Lemmatized + No Stopwords)', color='skyblue', alpha=0.5)\n",
    "\n",
    "plt.title('KDE words per comment: before and after')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Normalized Words: {df_supervised['n_words_original'].mean():.2f}\")\n",
    "print(f\"Mean Clean Words: {df_supervised['n_words_clean'].mean():.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fc939870edcf3d2",
   "metadata": {},
   "source": [
    "def get_top_words(body, n=20):\n",
    "    all_body = ' '.join(body.fillna(''))\n",
    "    words = all_body.split()\n",
    "    return pd.DataFrame(Counter(words).most_common(n), columns=['word', 'count'])\n",
    "\n",
    "top_normalized = get_top_words(df_supervised['body_normalized'])\n",
    "top_clean = get_top_words(df_supervised['body_clean'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.barplot(data=top_normalized, x='count', y='word', hue='word', ax=axes[0], palette='Reds_r', legend=False)\n",
    "axes[0].set_title('Top 20 Words (Normalized)')\n",
    "\n",
    "sns.barplot(data=top_clean, x='count', y='word', hue='word', ax=axes[1], palette='Blues_r', legend=False)\n",
    "axes[1].set_title('Top 20 Words (Cleaned & Lemmized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bf72b365a158f3d",
   "metadata": {},
   "source": [
    "vocab_normalized = set(' '.join(df_supervised['body_normalized'].astype(str)).split())\n",
    "vocab_clean = set(' '.join(df_supervised['body_clean'].astype(str)).split())\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'Stage': ['Normalized', 'Cleaned'],\n",
    "    'Unique Words (Vocabulary)': [len(vocab_normalized), len(vocab_clean)]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(\n",
    "    data=metrics,\n",
    "    y='Unique Words (Vocabulary)',\n",
    "    hue='Stage',\n",
    "    palette='viridis',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Vocabulary Dimension Reduction')\n",
    "for index, row in metrics.iterrows():\n",
    "    plt.text(\n",
    "        index,\n",
    "        row['Unique Words (Vocabulary)'],\n",
    "        f\"{row['Unique Words (Vocabulary)']}\",\n",
    "        color='black',\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "506696e0c084a0e3",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_non_alphanumeric(text_series):\n",
    "    anomalies = Counter()\n",
    "    text_list = text_series.fillna('').astype(str).tolist()\n",
    "    for text in text_list:\n",
    "        for char in text:\n",
    "            if not char.isalpha() and not char.isspace():\n",
    "                anomalies[char] += 1\n",
    "    return anomalies\n",
    "\n",
    "anomalie_trovate = check_non_alphanumeric(df_supervised['body_clean'])\n",
    "\n",
    "print(\"\\n--- CHECCKKK!!! ---\")\n",
    "print(f\"Found {len(anomalie_trovate)} NON-alphabetic characters\")\n",
    "for char, count in anomalie_trovate.most_common():\n",
    "    print(f\"   Char: [{char}]  -  Compairs: {count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41c3c0a8",
   "metadata": {},
   "source": [
    "## 3 Text Vectorization.\n",
    "\n",
    "*a. Only for the supervised task (data_supervised.csv): Group and join all comments of the\n",
    "same author, creating a “new” dataset to be used for the supervised task (Section 2).*\n",
    "\n",
    "*b: As ML algorithms struggle to handle directly the raw textual data. You are required to\n",
    "convert the text into numerical representations (vectors) through Bag of Words (BoW).*\n",
    "\n",
    "*c: Another way to assign a vector representation to a word is to associate the TF-IDF\n",
    "representation (Term Frequency-Inverse Document Frequency) to each user/comment.\n",
    "Can you observe and explain the differences between the numerical representations\n",
    "generated by BoW and TF-IDF?*\n",
    "\n",
    "\n",
    "*A Wrap up section at the bottom describing which files are created and where by this notebook is at the very end of the notebook. To change the input file instead view the very first cell of the notebook. Originally meant for only the supervised dataset as described by the requirements.*\n",
    "\n",
    "For problems contact Matteo Sottocornola on Telegram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891fbc2",
   "metadata": {},
   "source": [
    "### Part 1 of 1.3\n",
    "\n",
    "*Only for the supervised task (data_supervised.csv): Group and join all comments of the\n",
    "same author, creating a “new” dataset to be used for the supervised task (Section 2).*"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "9ccba70d",
   "metadata": {},
   "source": [
    "# TODO capire se aggiungere aggregated subreddit/created_utc\n",
    "\n",
    "# import as panda dataframe.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./clean_supervised.csv\") #In principio da usare solo su clean_supervised.\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "a640eb50",
   "metadata": {},
   "source": [
    "#Drop the two unneeded columns inside clean_supervised\n",
    "#Rename the cleaned body to just body cause I prefer that way.\n",
    "df = df.drop(columns=['body','body_normalized'])\n",
    "df = df.rename(columns={'body_clean':'body'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "b75bba49",
   "metadata": {},
   "source": [
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "93cdc3e0",
   "metadata": {},
   "source": [
    "#Non sicuro se anche questi andrano tenuti/concatenati come body quindi drop per ora.\n",
    "df_text_only = df.drop(columns=['created_utc','subreddit'])\n",
    "print(df_text_only)\n",
    "\n",
    "#nella parte 2 di Rocco ci sono testi che diventano vuoti (e giusto cosi erano interamente composti da stop words).\n",
    "#Qui li sto semplicemente togliendo. Da notare che alcuni utenti vanno scartati\n",
    "#perche non hanno piu nessun commento.\n",
    "print(df_text_only.shape)\n",
    "df_text_only = df_text_only.dropna(subset=['body']) #perdiamo un 6000 su 296,000 posts.\n",
    "print(df_text_only.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "3dba83b1",
   "metadata": {},
   "source": [
    "#come richiesto per ogni autore dobbiamo avere una sola riga con tutti i body concatenati.\n",
    "df_grouped = df_text_only.groupby('author')['body'].apply(\" \".join).reset_index()\n",
    "print(df_grouped)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "0cc35469",
   "metadata": {},
   "source": [
    "#Quick sanity check.\n",
    "i=4 #select an index, and as such a user.\n",
    "user = df.iloc[i,0]\n",
    "print(\"user: \", user,\" posted this: \", df.iloc[i, 3])\n",
    "\n",
    "print(df.groupby(\"author\").size().loc[user] )\n",
    "#df.groupby(\"author\").count()['author'=df.iloc[0, 0]]\n",
    "\n",
    "row = df_grouped[df_grouped[\"author\"] == user] #user\n",
    "#print(row.iloc[0,1]) \n",
    "#You should see in the last print somewhere within the text a copy of the previously printed body."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "3b95236e",
   "metadata": {},
   "source": [
    "#Convertiamo ad un numpy  per ragioni di efficenza/memoria.\n",
    "numpy_grouped = df_grouped[[\"author\",\"body\"]].to_numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f8d2bd4",
   "metadata": {},
   "source": [
    "### Part 2 of 1.3\n",
    "*B: As ML algorithms struggle to handle directly the raw textual data. You are required to\n",
    "convert the text into numerical representations (vectors) through Bag of Words (BoW).*\n",
    "\n",
    "*Bag of Words (BoW) is a technique widely used to transform textual data into machine-readable format, specifically numerical values, without considering grammar and word order.*\n",
    "\n",
    "*We will be counting the occurence of every word in the vocabulary we use. Where the word was and it's actual structure is lost. Basically you are adding a new column for each word that is in our dataset and adding the number of times it was used for each row.*\n",
    "\n",
    "*Note that executing 1.2 to remove stop words first is heavily recomended to reduce the number of words and hence attributes we get with BoW.*"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "29a9df27",
   "metadata": {},
   "source": [
    "#useful example of BoW\n",
    "#https://www.datacamp.com/tutorial/python-bag-of-words-model?dc_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "#df_grouped = df_grouped[:400] #remove after 1.2 available, done to reduce complexity for now.\n",
    "\n",
    "\n",
    "#Dobbiamo creare un vocabolario di tutte le parole usate nei svariati body.\n",
    "# Function to preprocess and tokenize\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize: split the text into words\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "for sentence in numpy_grouped[:, 1]:\n",
    "    tokens = preprocess(sentence)\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "vocabulary = sorted(vocabulary)\n",
    "\n",
    "print(len(vocabulary))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "b8f17b6f",
   "metadata": {},
   "source": [
    "#Create a dictionary so future lookups are O(1)\n",
    "word2idx = {w:i for i,w in enumerate(vocabulary)}\n",
    "\n",
    "def create_bow_vector(sentence):\n",
    "    vector = [0] * len(word2idx)  # Initialize a vector of zeros\n",
    "    for word in sentence:\n",
    "        idx = word2idx.get(word)  # Find the index of the word in the vocabulary using dictionary.\n",
    "        if idx is not None:\n",
    "            vector[idx] += 1  # Increment the count at that index\n",
    "    return vector\n",
    "\n",
    "#initialize numpy_bow\n",
    "numpy_bow = np.zeros((numpy_grouped.shape[0], len(vocabulary)), dtype=np.int32)\n",
    "\n",
    "# Create BoW vector for each author, i.e. for each body in our numpy_grouped.\n",
    "for i ,raw_text in enumerate(numpy_grouped[:,1]):\n",
    "    tokens = preprocess(raw_text) #tokenize.\n",
    "    temp = create_bow_vector(tokens)\n",
    "    numpy_bow[i, :] = temp\n",
    "\n",
    "#Lento perche troppe parole nel vocabulary."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "0d6d589d",
   "metadata": {},
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "# most but not all will be zeroes, that's normal."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "1c9ced30",
   "metadata": {},
   "source": [
    "#Ci servira scrivere da qualche parte anche autori per ricordarci a chi corrisponde.\n",
    "authors = numpy_grouped[:, 0]   # shape (n_auth,)\n",
    "\n",
    "print(authors.shape)\n",
    "print(numpy_bow.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "43b59a65",
   "metadata": {},
   "source": [
    "#np.savetxt(\"1.3-bow-authors2.csv\", authors, fmt=\"%s\", delimiter=\",\")\n",
    "#np.savetxt(\"1.3-bow_matrix.csv\", numpy_bow, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "np.save(\"1.3-bow_matrix.npy\", numpy_bow)\n",
    "np.save(\"1.3-bow-authors.npy\", authors)\n",
    "#poi si legge cosi.\n",
    "#numpy_bow = np.load(\"bow_matrix.npy\")\n",
    "#authors = np.load(\"authors.npy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29e94561",
   "metadata": {},
   "source": [
    "### Part 3 of 1.3 - TF-IDF\n",
    "\n",
    "*Another way to assign a vector representation to a word is to associate the TF-IDF\n",
    "representation (Term Frequency-Inverse Document Frequency) to each user/comment.\n",
    "Can you observe and explain the differences between the numerical representations\n",
    "generated by BoW and TF-IDF?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d50409",
   "metadata": {},
   "source": [
    "*For TF-IDF it is necessary to create the vocabulary of all the distinct words and then for each word performing the following calculation which requires computing these two first...*\n",
    "\n",
    "![Fig1.png](Fig1.png)\n",
    "\n",
    "\n",
    "*In practice for every word A and each user we calculate two metrics, one is what percentage of overall users used word A (log of this number). And the other is what percentage of the words used by that specific user correspond to A. The index is then the multiple of these two.*\n",
    "\n",
    "*It's meant to provide a measure of how important each word is to that text, but corrected to cancel out words that are just common in general and not common to this specific text.*"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "5fdc1db0",
   "metadata": {},
   "source": [
    "#We reuse the vocabulary computed for the previous section so execute that first.\n",
    "print(len(vocabulary))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "97ce20ff",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Count in how many documents each word appears\n",
    "df_counter = Counter()\n",
    "\n",
    "# Create BoW vector for each author, i.e. for each body in our numpy_grouped.\n",
    "for raw_text in numpy_grouped[:,1]:\n",
    "    tokens = preprocess(raw_text) #tokenize.\n",
    "    unique_words = set(tokens)\n",
    "    df_counter.update(unique_words)\n",
    "\n",
    "N = authors.shape[0]            # number of documents\n",
    "\n",
    "idf = [math.log(N / df_counter.get(word, 1)) for word in vocabulary]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "ca0788a9",
   "metadata": {},
   "source": [
    "print(type(idf))\n",
    "print(len(idf))\n",
    "print(len(vocabulary))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "65cd2986",
   "metadata": {},
   "source": [
    "#now to compute the second metric TF and directly the TF-IDF\n",
    "#word2idx from earlier is re used.\n",
    "\n",
    "def create_TF_IDF_vector(sentence, idf):\n",
    "    vector = [0] * len(idf)  # If not present in user's comment then TF-IDF is 0.\n",
    "    \n",
    "    for word in sentence:\n",
    "        idx = word2idx.get(word)  # Find the index of the word in the vocabulary\n",
    "        if idx is not None:\n",
    "            vector[idx] += 1  # Increment the count at that index\n",
    "    \n",
    "    \n",
    "    for i in range(len(idf)):\n",
    "        vector[i] = vector[i] / len(sentence)\n",
    "        vector[i] = vector[i] * idf[i]\n",
    "    \n",
    "    return vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "8ca927e8",
   "metadata": {},
   "source": [
    "# Preallocate the full TF-IDF matrix\n",
    "TFIDF_vectors = np.zeros((numpy_grouped.shape[0], len(idf)), dtype=float)\n",
    "\n",
    "\n",
    "#TFIDF_vectors = []\n",
    "\n",
    "# Create TF-IDF vector for each sentence in the processed corpus\n",
    "for i, raw_text in enumerate(numpy_grouped[:,1]):\n",
    "    tokens = preprocess(raw_text) #tokenize.\n",
    "    #TFIDF_vectors.append(create_TF_IDF_vector(tokens,idf))\n",
    "    temp = create_TF_IDF_vector(tokens, idf)\n",
    "    TFIDF_vectors[i, :] = temp\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "da59da51",
   "metadata": {},
   "source": [
    "print(\"Single DF-IDF row\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#print(TFIDF_vectors[17])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "6eb56807",
   "metadata": {},
   "source": [
    "#Debug - checks\n",
    "ind = 0\n",
    "for el in TFIDF_vectors[1]:\n",
    "    if el != 0.0:\n",
    "        print(el, ind)\n",
    "    ind = ind + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "27e5d1ab",
   "metadata": {},
   "source": [
    "#Debug - checks\n",
    "print(idf[36451]*0.5, idf[79596]*0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "248f42ac",
   "metadata": {},
   "source": [
    "ind = 0\n",
    "for el in TFIDF_vectors[18]:\n",
    "    if el != 0.0:\n",
    "        print(el, ind)\n",
    "        print(\"index: \", ind, \" TF-IDF: \", el, \" for word: \", vocabulary[ind])\n",
    "    ind = ind + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": null,

   "id": "1e69dbb7",
   "metadata": {},
   "source": [
    "#np.savetxt(\"1.3-tf_idf-authors.csv\", authors, fmt=\"%s\", delimiter=\",\")\n",
    "#np.savetxt(\"1.3-tf_idf_matrix.csv\", TFIDF_vectors, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "np.save(\"1.3-tf-idf-matrix.npy\", TFIDF_vectors)\n",
    "np.save(\"1.3-tf-idf-authors.npy\", authors)\n",
    "#poi si legge cosi.\n",
    "#numpy_bow = np.load(\"bow_matrix.npy\")\n",
    "#authors = np.load(\"authors.npy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cbb30c4",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "with open(\"1.3-bow-vocabulary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(vocabulary)\n",
    "#check saved correctly\n",
    "#with open(\"1.3-bow-vocabulary.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    vocabulary2 = next(reader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86a85012",
   "metadata": {},
   "source": [
    "#check saved correctly\n",
    "#print(vocabulary)\n",
    "#print(vocabulary2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57513a35",
   "metadata": {},
   "source": [
    "## 4 Reduce the complexity of your dataset.\n",
    "*Threshold-based: BoW and TF-IDF representations use the same large vocabulary, which\n",
    "can result in a very high-dimensional feature space. This may lead to issues like the curse of\n",
    "dimensionality or even out-of-memory errors when processing the data. Use the BoW\n",
    "representation to identify thresholds to reduce the dimensionality of the feature space of\n",
    "either the BoW itself or the TF-IDF representation.*\n",
    "\n",
    "*Hint: Consider removing words that are either too common or too rare, as they often carry\n",
    "less useful information. One approach is to filter out words whose frequencies fall below a\n",
    "lower quantile (Ql) or above an upper quantile (Qu).*\n",
    "\n",
    "*Dimensionality Reduction: Try applying PCA to reduce the dimensionality of your features.\n",
    "Does it work? How many components would you keep and why? Do the components you\n",
    "obtain with PCA have a physical interpretation?*\n",
    "*Note: The number of features is very large and both TF-IDF and BoW matrix are sparse.\n",
    "Suggestion: Do not scale your dataset with StandardScaler. Indeed, subtracting the mean\n",
    "from each feature will render the sparse matrix a dense one, drastically increasing memory\n",
    "usage. Try working with the sparse format using scipy.sparse and apply Truncated SVD,\n",
    "which is designed for sparse data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316662c",
   "metadata": {},
   "source": [
    "### Threshold-based"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 7,

   "id": "b1679e30-5973-4786-ae9d-88c62ad50641",
   "metadata": {},
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": 8,

   "id": "ecbf2921",
   "metadata": {},
   "source": [
    "bow_matr = np.load('1.3-bow_matrix.npy') \n",
    "authors = np.load('1.3-bow-authors.npy', allow_pickle=True) \n",
    "\n",
    "# print utilities\n",
    "print(f\"Original dimension: {bow_matr.shape}\")\n",
    "print(f\"Type matrix: {type(bow_matr)}\")\n",
    "\n",
    "# converting in sparse matrix\n",
    "bow_sparse = sparse.csr_matrix(bow_matr)\n",
    "print(f\"Dimensione sparsa: {bow_sparse.shape}\")\n",
    "\n",
    "\n",
    "# compute frequency of words (sum per column)\n",
    "word_freq = np.array(bow_sparse.sum(axis=0)).flatten()\n",
    "\n",
    "# some stats\n",
    "print(f\"Min freq: {word_freq.min()}\")\n",
    "print(f\"Max freq: {word_freq.max()}\")\n",
    "print(f\"Mean freq: {word_freq.mean():.2f}\")\n",
    "print(f\"Median freq: {np.median(word_freq):.2f}\")\n",
    "\n",
    "# percentiles\n",
    "percentiles = [0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]\n",
    "print(\"\\nPercentiles distribution:\")\n",
    "for p in percentiles:\n",
    "    print(f\"  {p*100:2.0f}%: {np.quantile(word_freq, p):.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ca80908-d9f2-44f3-a8b7-f7a254584ba6",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 9,

   "id": "1cd26752-ab9e-4e54-b79e-ebcf6ce9e649",
   "metadata": {},
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Log distr\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(np.log1p(word_freq), bins=100, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Log(Freq + 1)')\n",
    "plt.ylabel('Num of words')\n",
    "plt.title('Distribution (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(word_freq, showfliers=False)\n",
    "plt.ylabel('Freq')\n",
    "plt.title('Boxplot freq')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom on low freq\n",
    "plt.subplot(1, 3, 3)\n",
    "low_freq = word_freq[word_freq <= 50]  # Solo frequenze <= 50 per vedere meglio\n",
    "plt.hist(low_freq, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Freq')\n",
    "plt.ylabel('Num of words')\n",
    "plt.title('Zoom on low frequencies (<=50)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f0061fe-b43d-4273-91e7-1902bacd1541",
   "metadata": {},
   "source": [
    "### FIltering based on quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,

   "id": "379b7a8f-8ceb-43df-b064-d0e11da63731",
   "metadata": {},
   "source": [
    "#TODO togliere il Q_lower?\n",
    "\n",
    "Q_lower = 0.01\n",
    "Q_upper = 0.99\n",
    "\n",
    "lower_threshold = np.quantile(word_freq, Q_lower)\n",
    "upper_threshold = np.quantile(word_freq, Q_upper)\n",
    "\n",
    "print(f\"Quantilies choosen: {Q_lower} - {Q_upper}\")\n",
    "#print(f\"Lower threshold (deleting words with freq <= {lower_threshold:.2f})\")\n",
    "print(f\"Upper threshold (deleting words with freq >= {upper_threshold:.2f})\")\n",
    "\n",
    "# Apply the filtering\n",
    "words_to_keep = (word_freq > lower_threshold) & (word_freq < upper_threshold)\n",
    "#words_to_keep = (word_freq < upper_threshold)\n",
    "bow_filtered = bow_sparse[:, words_to_keep]\n",
    "\n",
    "# Stats of filtering\n",
    "#words_removed_lower = (word_freq <= lower_threshold).sum()\n",
    "words_removed_upper = (word_freq >= upper_threshold).sum()\n",
    "words_kept = words_to_keep.sum()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Words before: {len(word_freq)}\")\n",
    "#print(f\"Removed rearest words: {words_removed_lower} ({words_removed_lower/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Removed common words: {words_removed_upper} ({words_removed_upper/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words keeped: {words_kept} ({words_kept/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Dim reducted: {(1 - words_kept/len(word_freq))*100:.1f}%\")\n",
    "print(f\"Final BoW matrix: {bow_filtered.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cfa6fc3-8204-4709-a62b-bed764536333",
   "metadata": {},
   "source": [
    "### Showing results"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 11,

   "id": "9f8ffbcb-a683-4ea4-84a9-656252efc250",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Before filtering\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(np.log1p(word_freq), bins=100, alpha=0.6, color='lightblue', label='all words')\n",
    "plt.axvline(np.log1p(lower_threshold), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Threshold inferiore ({lower_threshold:.1f})')\n",
    "plt.axvline(np.log1p(upper_threshold), color='orange', linestyle='--', linewidth=2, \n",
    "            label=f'Threshold superiore ({upper_threshold:.1f})')\n",
    "plt.xlabel(\"Log(freq + 1)\")\n",
    "plt.ylabel(\"Num of words\")\n",
    "plt.title(\"Before filtering\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# After filtering\n",
    "plt.subplot(1, 2, 2)\n",
    "word_freq_filtered = word_freq[words_to_keep]\n",
    "plt.hist(np.log1p(word_freq_filtered), bins=50, alpha=0.6, color='lightgreen')\n",
    "plt.axvline(np.log1p(lower_threshold), color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "plt.axvline(np.log1p(upper_threshold), color='orange', linestyle='--', linewidth=2, alpha=0.7)\n",
    "plt.xlabel(\"Log(freq + 1)\")\n",
    "plt.ylabel(\"Num of words\")\n",
    "plt.title(\"After filtering\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",

   "execution_count": 12,

   "id": "3a29f85f",
   "metadata": {},
   "source": [
    "#TODO vogliamo aggiungere questo histograma senza log per il dopo?\n",
    "#TODO da notare che e giusto lasciare quello in scala log per il confronto sopra.\n",
    "plt.hist(word_freq_filtered, bins=50, alpha=0.6, color='lightgreen')\n",
    "plt.xlabel(\"freq\")\n",
    "plt.ylabel(\"Num of words\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce07b5d7-2647-4c7e-aa87-63279d03997a",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "3a40be71-7943-4451-a46c-766d668bfd32",
   "metadata": {},
   "outputs": [],

   "source": [
    "# saving results\n",
    "sparse.save_npz('1.4-bow_filtered_quantile.npz', bow_filtered)\n",
    "np.save('1.4-words_kept_indices.npy', words_to_keep) #TODO questo si puo sopprimere secondo me. A che ci serve ricordarci che parola corrisponde a cosa?\n",
    "\n",
    "print(f\"Files saved:\")\n",
    "print(f\"1.4-bow_filtered_quantile.npz - Matrice BoW filtered\")\n",
    "print(f\"1.4-words_kept_indices.npy - index words kept\")\n",
    "\n",
    "tfidf_matrix = np.load('1.3-tf-idf-matrix.npy')\n",
    "tfidf_sparse = sparse.csr_matrix(tfidf_matrix)\n",
    "tfidf_filtered = tfidf_sparse[:, words_to_keep]\n",
    "sparse.save_npz('1.4-tfidf_filtered_quantile.npz', tfidf_filtered)\n",
    "print(f\"1.4-tfidf_filtered_quantile.npz - Matrix TF-IDF filtered\")\n",
    "print(f\"Dimension filtered TF-IDF: {tfidf_filtered.shape}\")\n",
    "\n",
    "print(f\"Dataset: {bow_sparse.shape[1]} features\")\n",
    "print(f\"Dataset filtered: {bow_filtered.shape[1]} features\") \n",
    "print(f\"Final reduction: {(1 - bow_filtered.shape[1]/bow_sparse.shape[1])*100:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4dba55df-df91-4117-b29b-28ba458e3c0d",
   "metadata": {},
   "source": [
    "## Appling the Truncated Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d45d327-9a46-44df-8936-5cb480d80900",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed8ebcd6-a26d-49a2-adb8-a961934fe515",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f679f45-0958-4bef-b3cf-b5c2fd2b5af2",
   "metadata": {},
   "source": [
    "#TODO lo so che dopo finiamo con proprio pochi attributi ma sicuro che\n",
    "#non va applicato al dataset gia filtrato per quantili?\n",
    "# load data\n",
    "bow_matrix = np.load('1.3-bow_matrix.npy') \n",
    "bow_sparse = sparse.csr_matrix(bow_matrix)\n",
    "print(f\"BoW dimension: {bow_sparse.shape}\")\n",
    "\n",
    "tfidf_matrix = np.load('1.3-tf-idf-matrix.npy')\n",
    "tfidf_sparse = sparse.csr_matrix(tfidf_matrix)\n",
    "print(f\"TF-IDF dimension: {tfidf_sparse.shape}\")\n",
    "has_tfidf = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3776cd6c-07cd-48ff-adc5-00224f23af46",
   "metadata": {},
   "source": [
    "#TODO sicuro che serve salvarli?\n",
    "# using sparse matrix\n",
    "sparse.save_npz(\"bow_sparse.npz\", bow_sparse)\n",
    "sparse.save_npz(\"tfidf_sparse.npz\", tfidf_sparse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37d9eb6b-c0a2-46f3-85a6-4f20a8d27d68",
   "metadata": {},
   "source": [
    "bow_sparse1 = sparse.load_npz(\"bow_sparse.npz\")\n",
    "print(\"BoW dimensions:\", bow_sparse.shape)\n",
    "\n",
    "tfidf_sparse1 = sparse.load_npz(\"tfidf_sparse.npz\")\n",
    "print(\"TF-IDF dimensions:\", tfidf_sparse.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd3131cd-4ed1-414a-afe2-c2bf2b1ce1d0",
   "metadata": {},
   "source": [
    "# k components keeped \n",
    "k = 100\n",
    "\n",
    "# truncated svd on matrix to reducing the complexity\n",
    "svd= TruncatedSVD(n_components=k, random_state=43)\n",
    "\n",
    "# normalization of vectors\n",
    "normalizer = Normalizer(copy =False)\n",
    "\n",
    "# pipeline: svd and then normalization\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "# fit transform on BoW sparse matrix \n",
    "X_svd = lsa.fit_transform(bow_sparse1)\n",
    "\n",
    "print(f\"New shape after Truncated SVD: {X_svd.shape}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da31f2bd-4fd3-4701-b09e-a24c1dc7f62f",
   "metadata": {},
   "source": [
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_.sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c9fb523-65fc-4bb4-9edc-15f9486c32f3",
   "metadata": {},
   "source": [
    "#TODO credo che manca qualcosa, volevi provare con diversi k e fare il plot?\n",
    "\n",
    "# visualization of variance curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_components_options, explained_variances, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.title('Explained variance curve - Truncated SVD')\n",
    "plt.grid(True, alpha=0.3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8423ee44-8916-467c-b116-65ac38e689b3",
   "metadata": {},
   "source": [
    "# Guide lines per 8'% - 90% of explained variance\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% of variance')\n",
    "plt.axhline(y=0.9, color='g', linestyle='--', alpha=0.7, label='90% of variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee68b759-edff-4a2c-bb27-4daee883925b",
   "metadata": {},
   "source": [
    "# appling the truncated SVD\n",
    "\n",
    "# choosing the number of components based of variance analysis\n",
    "n_components_final = 1000  \n",
    "\n",
    "print(f\"Truncated SVD with {n_components_final} components...\")\n",
    "\n",
    "# For BoW\n",
    "svd_bow = TruncatedSVD(n_components=n_components_final, random_state=42)\n",
    "bow_svd = svd_bow.fit_transform(bow_sparse)\n",
    "print(f\"BoW after SVD: {bow_svd.shape}\")\n",
    "\n",
    "# For TF-IDF\n",
    "svd_tfidf = TruncatedSVD(n_components=n_components_final, random_state=42)\n",
    "tfidf_svd = svd_tfidf.fit_transform(tfidf_sparse)\n",
    "print(f\"TF-IDF after SVD: {tfidf_svd.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96819060-799a-4e92-9673-d99261075880",
   "metadata": {},
   "source": [
    "# svd analysisi\n",
    "print(\"First 10 components BoW - explained variance:\")\n",
    "for i, var in enumerate(svd_bow.explained_variance_ratio_[:10]):\n",
    "    print(f\"  Componente {i+1}: {var:.4f}\")\n",
    "\n",
    "# Total explained variance\n",
    "total_variance_bow = svd_bow.explained_variance_ratio_.sum()\n",
    "print(f\"\\ntotal explained variance BoW: {total_variance_bow:.3f}\")\n",
    "\n",
    "# Most important features\n",
    "def print_top_features(svd_model, feature_names, n_features=10, n_components=3):\n",
    "    \"\"\"Stampa le feature più importanti per le prime componenti\"\"\"\n",
    "    components = svd_model.components_\n",
    "    for i in range(n_components):\n",
    "        print(f\"\\nComponente {i+1} - Top features:\")\n",
    "        # Ottieni gli indici delle feature con peso assoluto più alto\n",
    "        top_indices = np.argsort(np.abs(components[i]))[-n_features:][::-1]\n",
    "        for idx in top_indices:\n",
    "            # Nota: qui non abbiamo i nomi delle feature reali\n",
    "            print(f\"  Feature {idx}: peso {components[i][idx]:.4f}\")\n",
    "\n",
    "# Nota: Non abbiamo i nomi delle feature, quindi mostriamo solo gli indici\n",
    "print(\"\\nTop features per prime 3 componenti BoW:\")\n",
    "print(\"(Nota: senza nomi feature reali, mostro solo indici)\")\n",
    "for i in range(3):\n",
    "    top_indices = np.argsort(np.abs(svd_bow.components_[i]))[-10:][::-1]\n",
    "    print(f\"Componente {i+1}: features {top_indices}\")\n",
    "\n",
    "#TODO la list vocabulary ce l'hai ancora ed e nello stesso ordine.\n",
    "for i in range(3):\n",
    "    top_indices = np.argsort(np.abs(svd_bow.components_[i]))[-10:][::-1]\n",
    "    print(f\"Componente {i+1}: features\", end=\" \")\n",
    "    for idx in top_indices:\n",
    "        print(vocabulary[idx], end=\" \")\n",
    "    print(\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a4cffaa-f265-4192-896c-e0e211794200",
   "metadata": {},
   "source": [
    "# Saving matrix\n",
    "np.save('1.4-bow_svd_reduced.npy', bow_svd)\n",
    "np.save('1.4-tfidf_svd_reduced.npy', tfidf_svd)\n",
    "\n",
    "# Salva i modelli SVD #TODO sicuro che vogliamo salvare i modelli abbiamo di piu in piu file in sta cartella.\n",
    "import joblib\n",
    "joblib.dump(svd_bow, '1.4-svd_bow_model.pkl')\n",
    "joblib.dump(svd_tfidf, '1.4-svd_tfidf_model.pkl')\n",
    "\n",
    "print(f\"Original: {bow_sparse.shape} -> reducted: {bow_svd.shape}\")\n",
    "print(f\"Reduction: {(1 - bow_svd.shape[1]/bow_sparse.shape[1])*100:.1f}%\")\n",
    "print(f\"Preserved variance: {total_variance_bow*100:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4945500a-5f9c-48ed-bec4-8f2c79ca3c5b",
   "metadata": {},
   "source": [
    "# differences between truncated svd and threshold\n",
    "try:\n",
    "    bow_filtered = sparse.load_npz('1.4-bow_filtered_quantile.npz')\n",
    "    print(\"CONFRONTO FINALE:\")\n",
    "    print(f\"Threshold-based: {bow_filtered.shape} (mantiene parole reali)\")\n",
    "    print(f\"Truncated SVD:    {bow_svd.shape} (componenti latenti)\")\n",
    "    print(\"\\nVantaggi Threshold-based:\")\n",
    "    print(\"  • Features interpretabili (parole reali)\")\n",
    "    print(\"  • Mantiene semantica originale\")\n",
    "    print(\"\\nVantaggi Truncated SVD:\")\n",
    "    print(\"  • Riduzione più aggressiva\")\n",
    "    print(\"  • Cattura correlazioni tra features\")\n",
    "    print(\"  • Meno sensibile al rumore\")\n",
    "    \n",
    "except:\n",
    "    print(\"Dati threshold-based non trovati per confronto\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
