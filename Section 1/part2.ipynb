{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning and Text Standardization.\n",
    "\n",
    "a. Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case).\n",
    "If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n",
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry\n",
    "very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in\n",
    "reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK\n",
    "or spaCy for tokenization).\n"
   ],
   "id": "565b2f5b22d23b8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:13:34.729685Z",
     "start_time": "2025-11-20T10:13:34.089717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import html\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "from langdetect import detect"
   ],
   "id": "9e592ac05c433f0a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:13:39.147567Z",
     "start_time": "2025-11-20T10:13:34.747848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_supervised   = pd.read_csv( \"data_supervised.csv\")\n",
    "df_unsupervised = pd.read_csv( \"data_unsupervised.csv\")\n",
    "df_target       = pd.read_csv( \"target_supervised.csv\")\n",
    "\n",
    "print(df_supervised.shape, df_unsupervised.shape, df_target.shape)"
   ],
   "id": "4df2ccef1de5a080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296042, 4) (1107946, 4) (5000, 2)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case). If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n"
   ],
   "id": "a26f4b6d952e2147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:13:54.401550Z",
     "start_time": "2025-11-20T10:13:39.216383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "remove_pattern = r'https?://\\S+|www\\.\\S+|r/\\w+|u/\\w+'\n",
    "\n",
    "df_supervised['body_normalized'] = (\n",
    "    df_supervised['body']\n",
    "    .fillna('')                                     # Gestisce i NaN\n",
    "    .astype(str)                                    # Assicura formato stringa\n",
    "    .str.lower()                                    # Case normalization (Punto a.)\n",
    "    .apply(html.unescape)                           # Decodifica HTML (es. &amp; -> &)\n",
    "    .str.replace(remove_pattern, ' ', regex=True) # Rimuove URL, r/, u/\n",
    "    .str.replace(r'\\s+', ' ', regex=True)           # Rimuove doppi spazi\n",
    "    .str.strip()                                    # Pulisce spazi inizio/fine\n",
    ")\n",
    "\n",
    "df_unsupervised['body_normalized'] = (\n",
    "    df_unsupervised['body']\n",
    "    .fillna('')\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .apply(html.unescape)\n",
    "    .str.replace(remove_pattern, ' ', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n"
   ],
   "id": "216898af0598672c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  \\\n",
      "0  I don't think we'd get nearly as much fanficti...   \n",
      "1  Thanks. I made it up, that's how I got over my...   \n",
      "2  Are you sure you aren't confusing Cyclops (the...   \n",
      "3                             dont do this to me bro   \n",
      "4        That's what we do when we can't find a mate   \n",
      "\n",
      "                                     body_normalized  \n",
      "0  i don't think we'd get nearly as much fanficti...  \n",
      "1  thanks. i made it up, that's how i got over my...  \n",
      "2  are you sure you aren't confusing cyclops (the...  \n",
      "3                             dont do this to me bro  \n",
      "4        that's what we do when we can't find a mate  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:13:54.521720Z",
     "start_time": "2025-11-20T10:13:54.476590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CHECKK!!!\n",
    "df_supervised[[\"body\", 'body_normalized']].head()"
   ],
   "id": "f3253b635dab3b99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                body  \\\n",
       "0  I don't think we'd get nearly as much fanficti...   \n",
       "1  Thanks. I made it up, that's how I got over my...   \n",
       "2  Are you sure you aren't confusing Cyclops (the...   \n",
       "3                             dont do this to me bro   \n",
       "4        That's what we do when we can't find a mate   \n",
       "\n",
       "                                     body_normalized  \n",
       "0  i don't think we'd get nearly as much fanficti...  \n",
       "1  thanks. i made it up, that's how i got over my...  \n",
       "2  are you sure you aren't confusing cyclops (the...  \n",
       "3                             dont do this to me bro  \n",
       "4        that's what we do when we can't find a mate  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>body_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "      <td>i don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "      <td>thanks. i made it up, that's how i got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "      <td>are you sure you aren't confusing cyclops (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dont do this to me bro</td>\n",
       "      <td>dont do this to me bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That's what we do when we can't find a mate</td>\n",
       "      <td>that's what we do when we can't find a mate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK or spaCy for tokenization)."
   ],
   "id": "70faf5d9b4a5b5b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:16:27.706088Z",
     "start_time": "2025-11-20T10:13:55.851390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text_full(text_series, batch_size=2000):\n",
    "    clean_texts = []\n",
    "\n",
    "    total_docs = len(text_series)\n",
    "\n",
    "    # tqdm show the process bar\n",
    "    for doc in tqdm(nlp.pipe(text_series, batch_size=batch_size), total=total_docs, desc=\"Processing\"):\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # 1. Filtering Stop Words e punctation (b)\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # 2. Take the lemma using spaCy (c)\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "\n",
    "    return clean_texts\n",
    "\n",
    "print(\"Elaboration of SUPERVISED dataset (smaller)...\")\n",
    "df_supervised['body_clean'] = process_text_full(df_supervised['body_normalized'].astype(str))\n",
    "\n",
    "print(\"Elaboration of UNSUPERVISED  dataset (bigger)...\")\n",
    "df_unsupervised['body_clean'] = process_text_full(df_unsupervised['body_normalized'].astype(str))"
   ],
   "id": "fdf78d7a566593f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborazione dataset SUPERVISED (più piccolo)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 62000/296042 [02:30<09:28, 411.62it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# --- ESECUZIONE ---\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mElaborazione dataset SUPERVISED (più piccolo)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m df_supervised[\u001B[33m'\u001B[39m\u001B[33mbody_clean\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mprocess_text_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_supervised\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbody_normalized\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mElaborazione dataset UNSUPERVISED (più grande)...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# Questo impiegherà tempo, ma alla fine avrai già finito sia il punto B che il punto C!\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mprocess_text_full\u001B[39m\u001B[34m(text_series, batch_size)\u001B[39m\n\u001B[32m      5\u001B[39m total_docs = \u001B[38;5;28mlen\u001B[39m(text_series)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# tqdm avvolge nlp.pipe per mostrarti la barra di progresso\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnlp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_series\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtotal_docs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mProcessing\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# 1. Filtra Stop Words e Punteggiatura (Punto b)\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\language.py:1620\u001B[39m, in \u001B[36mLanguage.pipe\u001B[39m\u001B[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001B[39m\n\u001B[32m   1618\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1619\u001B[39m         docs = pipe(docs)\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1761\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1751\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1752\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1753\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1758\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1759\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1760\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1761\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1762\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1763\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1764\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1761\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1751\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1752\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1753\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1758\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1759\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1760\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1761\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1762\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1763\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1764\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1761\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1751\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1752\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1753\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1758\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1759\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1760\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1761\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1762\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1763\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1764\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1708\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1706\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1707\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1708\u001B[39m     batch = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mitertools\u001B[49m\u001B[43m.\u001B[49m\u001B[43mislice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1709\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1710\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1761\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1751\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1752\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1753\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1758\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1759\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1760\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1761\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1762\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1763\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1764\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1708\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1706\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1707\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1708\u001B[39m     batch = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mitertools\u001B[49m\u001B[43m.\u001B[49m\u001B[43mislice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1709\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1710\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\language.py:1617\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1614\u001B[39m     docs = \u001B[38;5;28mself\u001B[39m._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001B[32m   1615\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1616\u001B[39m     \u001B[38;5;66;03m# if n_process == 1, no processes are forked.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1617\u001B[39m     docs = (\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ensure_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts)\n\u001B[32m   1618\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1619\u001B[39m         docs = pipe(docs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\language.py:1132\u001B[39m, in \u001B[36mLanguage._ensure_doc\u001B[39m\u001B[34m(self, doc_like)\u001B[39m\n\u001B[32m   1130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m doc_like\n\u001B[32m   1131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmake_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_like\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m   1134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m.vocab).from_bytes(doc_like)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\language.py:1124\u001B[39m, in \u001B[36mLanguage.make_doc\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1120\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) > \u001B[38;5;28mself\u001B[39m.max_length:\n\u001B[32m   1121\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1122\u001B[39m         Errors.E088.format(length=\u001B[38;5;28mlen\u001B[39m(text), max_length=\u001B[38;5;28mself\u001B[39m.max_length)\n\u001B[32m   1123\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\tokenizer.pyx:161\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\tokenizer.pyx:264\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._apply_special_cases\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\tokenizer.pyx:304\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._prepare_special_spans\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:500\u001B[39m, in \u001B[36mspacy.tokens.doc.Doc.__getitem__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:1350\u001B[39m, in \u001B[36mnormalize_slice\u001B[39m\u001B[34m(length, start, stop, step)\u001B[39m\n\u001B[32m   1348\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m start < \u001B[32m0\u001B[39m:\n\u001B[32m   1349\u001B[39m     start += length\n\u001B[32m-> \u001B[39m\u001B[32m1350\u001B[39m start = \u001B[38;5;28mmin\u001B[39m(length, \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1351\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1352\u001B[39m     stop = length\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
