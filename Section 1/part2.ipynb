{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565b2f5b22d23b8b",
   "metadata": {},
   "source": [
    "# Data Cleaning and Text Standardization.\n",
    "\n",
    "a. Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case).\n",
    "If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n",
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry\n",
    "very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in\n",
    "reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK\n",
    "or spaCy for tokenization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e592ac05c433f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:47.886503Z",
     "start_time": "2025-11-20T16:19:43.810882Z"
    }
   },
   "outputs": [],
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import html\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2ccef1de5a080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:47.891469300Z",
     "start_time": "2025-11-20T10:13:34.747848Z"
    }
   },
   "outputs": [],
   "source": [
    "df_supervised   = pd.read_csv(\"../data/data_supervised.csv\")\n",
    "df_unsupervised = pd.read_csv(\"../data/data_unsupervised.csv\")\n",
    "df_target       = pd.read_csv(\"../data/target_supervised.csv\")\n",
    "\n",
    "print(df_supervised.shape, df_unsupervised.shape, df_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f4b6d952e2147",
   "metadata": {},
   "source": [
    "Uniform text formats (e.g., case normalization, Hint: standardize the letters in lower case). If necessary, clean the comment text (e.g. URLs, subreddit refs, …).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216898af0598672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:47.892485900Z",
     "start_time": "2025-11-20T10:13:39.216383Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_pattern = r'https?://\\S+|www\\.\\S+|r/\\w+|u/\\w+'\n",
    "\n",
    "df_supervised['body_normalized'] = (\n",
    "    df_supervised['body']\n",
    "    .fillna('')                                     # Gestisce i NaN\n",
    "    .astype(str)                                    # Assicura formato stringa\n",
    "    .str.lower()                                    # Case normalization (Punto a.)\n",
    "    .apply(html.unescape)                           # Decodifica HTML (es. &amp; -> &)\n",
    "    .str.replace(remove_pattern, ' ', regex=True) # Rimuove URL, r/, u/\n",
    "    .str.replace(r'\\s+', ' ', regex=True)           # Rimuove doppi spazi\n",
    "    .str.strip()                                    # Pulisce spazi inizio/fine\n",
    ")\n",
    "\n",
    "df_unsupervised['body_normalized'] = (\n",
    "    df_unsupervised['body']\n",
    "    .fillna('')\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .apply(html.unescape)\n",
    "    .str.replace(remove_pattern, ' ', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3253b635dab3b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:47.894482800Z",
     "start_time": "2025-11-20T10:13:54.476590Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHECKK!!!\n",
    "df_supervised[[\"body\", 'body_normalized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70faf5d9b4a5b5b3",
   "metadata": {},
   "source": [
    "b. Stop words are not contributing much to our ML tasks, such as \"the\", \"a\", since they carry very little information. Take care of these kinds of words.\n",
    "\n",
    "c. Reduce words to their base or root form using Stemming/Lemmatization. This helps in reducing inflected words to a common base form. (Hint: Consider using libraries like NLTK or spaCy for tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf78d7a566593f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:19:47.898483Z",
     "start_time": "2025-11-20T10:13:55.851390Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_full(text_series, batch_size=2000):\n",
    "    clean_texts = []\n",
    "\n",
    "    total_docs = len(text_series)\n",
    "\n",
    "    # tqdm show the process bar\n",
    "    for doc in tqdm(nlp.pipe(text_series, batch_size=batch_size), total=total_docs, desc=\"Processing\"):\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # 1. Filtering Stop Words e punctation (b)\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                # 2. Take the lemma using spaCy (c)\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "\n",
    "    return clean_texts\n",
    "\n",
    "print(\"Elaboration of SUPERVISED dataset (smaller)...\")\n",
    "df_supervised['body_clean'] = process_text_full(df_supervised['body_normalized'].astype(str))\n",
    "\n",
    "df_supervised.to_csv(\"./clean_supervised.csv\", index=False)\n",
    "\n",
    "print(\"Elaboration of UNSUPERVISED  dataset (bigger)...\")\n",
    "df_unsupervised['body_clean'] = process_text_full(df_unsupervised['body_normalized'].astype(str))\n",
    "\n",
    "df_unsupervised.to_csv(\"./clean_unsupervised.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
